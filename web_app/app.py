#!/usr/bin/env python3
"""
Digital Twin Web Application - Enterprise Interface
FastAPI-based web application for document analysis, meeting processing, and productivity features
"""

from fastapi import FastAPI, File, UploadFile, Form, BackgroundTasks, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import uuid
import os
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
import logging
import httpx

# Configure logging first
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import our digital twin components
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from productivity_enhanced_twin import ProductivityEnhancedTwin
    logger.info("‚úÖ ProductivityEnhancedTwin imported successfully")
    PRODUCTIVITY_AVAILABLE = True
except ImportError as e:
    logger.warning(f"‚ùå ProductivityEnhancedTwin not available: {e}")
    ProductivityEnhancedTwin = None
    PRODUCTIVITY_AVAILABLE = False

try:
    from document_processor import SmartDocumentProcessor
    logger.info("‚úÖ SmartDocumentProcessor imported successfully")
    SMART_PROCESSING_AVAILABLE = True
except ImportError as e:
    logger.info(f"‚ÑπÔ∏è SmartDocumentProcessor not available: {e}")
    SmartDocumentProcessor = None
    SMART_PROCESSING_AVAILABLE = False
except Exception as e:
    logger.info(f"‚ÑπÔ∏è SmartDocumentProcessor setup issue: {e}")
    SmartDocumentProcessor = None
    SMART_PROCESSING_AVAILABLE = False

# Create FastAPI app
app = FastAPI(
    title="Digital Twin Enterprise Interface",
    description="Professional web interface for AI-powered productivity and document analysis",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add CSP middleware to allow inline scripts
@app.middleware("http")
async def add_csp_header(request: Request, call_next):
    response = await call_next(request)
    # Allow inline scripts and all localhost/127.0.0.1 connections
    csp_policy = (
        "script-src 'self' 'unsafe-inline' 'wasm-unsafe-eval' 'inline-speculation-rules' "
        "http://localhost:* http://127.0.0.1:* https://cdn.jsdelivr.net https://cdnjs.cloudflare.com; "
        "style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://cdnjs.cloudflare.com https://fonts.googleapis.com; "
        "font-src 'self' https://cdnjs.cloudflare.com https://fonts.gstatic.com; "
        "img-src 'self' data: https:; "
        "connect-src 'self' http://localhost:* http://127.0.0.1:*"
    )
    response.headers["Content-Security-Policy"] = csp_policy
    return response

# Setup templates and static files
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# Modern Dashboard Route
@app.get("/modern", response_class=HTMLResponse)
async def modern_dashboard(request: Request):
    """Serve the modern sleek dashboard"""
    return templates.TemplateResponse("modern_dashboard.html", {"request": request})

@app.get("/cockpit", response_class=HTMLResponse)
async def digital_cockpit(request: Request):
    """Serve the digital cockpit dashboard"""
    return templates.TemplateResponse("digital_cockpit.html", {"request": request})

# Global variables for task tracking
processing_tasks = {}
twin_instance = None
document_processor = None
enhanced_twin_instance = None

# Configuration
BEHAVIORAL_API_URL = "http://localhost:8000"

# Azure OpenAI Configuration for Jarvis
AZURE_OPENAI_ENDPOINT = "https://88f.openai.azure.com/"
AZURE_OPENAI_DEPLOYMENT = "88FGPT4o"
AZURE_OPENAI_API_VERSION = "2024-02-15-preview"
AZURE_OPENAI_API_KEY = "4f8768e63ff7402594c72809baf66ed4"

# Pydantic models
class ProcessingTask(BaseModel):
    task_id: str
    status: str  # pending, processing, completed, error
    action: str
    filename: str
    progress: int
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    created_at: datetime
    completed_at: Optional[datetime] = None
    original_filename: Optional[str] = None
    custom_title: Optional[str] = None

class ActionRequest(BaseModel):
    action: str
    content: str
    filename: Optional[str] = None
    user_id: Optional[str] = "default_user"
    custom_title: Optional[str] = None

class JarvisRequest(BaseModel):
    message: str
    context: Optional[str] = None
    user_id: str = "Paresh"

class ContentGenerationRequest(BaseModel):
    prompt: str
    type: str  # blog, linkedin, twitter, article
    title: str
    context: Optional[str] = None
    user_id: str = "Paresh"

async def initialize_twin():
    """Initialize the digital twin instance"""
    global twin_instance, document_processor
    
    # Initialize digital twin
    if twin_instance is None:
        if ProductivityEnhancedTwin:
            try:
                logger.info("Attempting to initialize ProductivityEnhancedTwin...")
                twin_instance = ProductivityEnhancedTwin()
                logger.info("‚úÖ Digital twin initialized successfully")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize digital twin: {e}")
                logger.error(f"Exception type: {type(e).__name__}")
                twin_instance = None
        else:
            logger.warning("‚ö†Ô∏è ProductivityEnhancedTwin class not available")
            twin_instance = None
    
    # Initialize smart document processor
    if document_processor is None:
        if SmartDocumentProcessor:
            try:
                logger.info("Attempting to initialize SmartDocumentProcessor...")
                azure_connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
                document_processor = SmartDocumentProcessor(
                    model_name="gpt-4",
                    max_chunk_tokens=4000,
                    azure_connection_string=azure_connection_string,
                    container_name="digital-twin-documents"
                )
                logger.info("‚úÖ Smart document processor initialized successfully")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Smart document processor initialization failed: {e}")
                document_processor = None
        else:
            logger.warning("‚ö†Ô∏è SmartDocumentProcessor class not available")

@app.on_event("startup")
async def startup_event():
    """Initialize application on startup"""
    await initialize_twin()
    
    # Create necessary directories
    os.makedirs("uploads", exist_ok=True)
    os.makedirs("results", exist_ok=True)

@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    """Enhanced dashboard with Chrome extension integration"""
    return templates.TemplateResponse("enhanced_dashboard.html", {
        "request": request,
        "title": "Enhanced Digital Twin Dashboard"
    })

@app.get("/smart", response_class=HTMLResponse)
async def smart_dashboard(request: Request):
    """Original smart dashboard interface"""
    return templates.TemplateResponse("smart_dashboard.html", {
        "request": request,
        "title": "Smart Digital Twin Enterprise"
    })

@app.get("/classic", response_class=HTMLResponse)
async def classic_dashboard(request: Request):
    """Classic dashboard interface"""
    return templates.TemplateResponse("dashboard.html", {
        "request": request,
        "title": "Digital Twin Enterprise Dashboard"
    })

@app.get("/v2", response_class=HTMLResponse)
async def smart_dashboard_v2(request: Request):
    """Next-generation unified intelligence workspace"""
    return templates.TemplateResponse("smart_dashboard_v2.html", {
        "request": request,
        "title": "Unified Intelligence Workspace - Digital Twin V2"
    })

@app.get("/voice_test", response_class=HTMLResponse)
async def voice_test(request: Request):
    """Voice intelligence test page"""
    return templates.TemplateResponse("voice_test.html", {
        "request": request,
        "title": "Voice Intelligence Test"
    })

@app.get("/mic_test", response_class=HTMLResponse)
async def mic_test(request: Request):
    """Simple microphone button test page"""
    return templates.TemplateResponse("mic_test.html", {
        "request": request,
        "title": "Microphone Button Test"
    })

@app.post("/upload")
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    action: str = Form(...),
    user_id: str = Form(default="default_user"),
    custom_title: str = Form(default="")
):
    """Upload file and start processing"""
    
    # Generate unique task ID
    task_id = str(uuid.uuid4())
    
    # Save uploaded file
    upload_dir = Path("uploads")
    upload_dir.mkdir(exist_ok=True)
    file_path = upload_dir / f"{task_id}_{file.filename}"
    
    try:
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)
        
        # Use custom title if provided, otherwise use filename
        display_title = custom_title.strip() if custom_title.strip() else file.filename
        
        # Create processing task
        task = ProcessingTask(
            task_id=task_id,
            status="pending",
            action=action,
            filename=display_title,
            progress=0,
            created_at=datetime.now(),
            original_filename=file.filename,
            custom_title=custom_title.strip() if custom_title.strip() else None
        )
        
        processing_tasks[task_id] = task
        
        # Start background processing
        background_tasks.add_task(process_document, task_id, file_path, action, user_id)
        
        return {"task_id": task_id, "status": "uploaded", "message": "Processing started"}
        
    except Exception as e:
        logger.error(f"Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.post("/process-text")
async def process_text(
    background_tasks: BackgroundTasks,
    request: ActionRequest
):
    """Process text content directly"""
    
    task_id = str(uuid.uuid4())
    
    # Use custom title if provided, otherwise use filename or default
    display_title = request.custom_title or request.filename or "text_input.txt"
    
    # Create processing task
    task = ProcessingTask(
        task_id=task_id,
        status="pending",
        action=request.action,
        filename=display_title,
        progress=0,
        created_at=datetime.now(),
        original_filename=request.filename,
        custom_title=request.custom_title
    )
    
    processing_tasks[task_id] = task
    
    # Start background processing
    background_tasks.add_task(process_text_content, task_id, request.content, request.action, request.user_id)
    
    return {"task_id": task_id, "status": "processing", "message": "Text processing started"}

async def process_document(task_id: str, file_path: Path, action: str, user_id: str):
    """Background task to process uploaded document"""
    
    # Try to initialize twin if not available
    if twin_instance is None:
        logger.warning("Digital twin not available, attempting to reinitialize...")
        await initialize_twin()
        
        if twin_instance is None:
            logger.error("Failed to initialize digital twin during document processing")
            processing_tasks[task_id].status = "error"
            processing_tasks[task_id].error_message = "Digital twin not available - initialization failed"
            return
    
    try:
        # Update status to processing
        processing_tasks[task_id].status = "processing"
        processing_tasks[task_id].progress = 10
        
        # Read file content based on file type
        content = await read_file_content(file_path)
        
        # Check if smart document processor is available and handle large documents
        if document_processor:
            try:
                # Check token count and process with smart chunking if needed
                token_count = document_processor.count_tokens(content)
                logger.info(f"üìä Document token count: {token_count:,} (limit: {document_processor.max_chunk_tokens:,})")
                
                if token_count > document_processor.max_chunk_tokens:
                    logger.info(f"üß© Large document detected ({token_count:,} tokens) - using smart chunking")
                    # Process with smart document processor
                    processed_doc = await document_processor.process_document(
                        content=content, 
                        filename=file_path.name,
                        store_in_azure=True
                    )
                    processing_tasks[task_id].processed_document = processed_doc
                    logger.info(f"‚úÖ Document chunked into {processed_doc.total_chunks} pieces")
                    
                    # Process chunks progressively
                    await process_document_chunks(task_id, processed_doc, action, user_id)
                    # Don't process normally - we've handled it with chunks
                    processing_tasks[task_id].progress = 90
                    processing_tasks[task_id].status = "completed"
                    processing_tasks[task_id].progress = 100
                    processing_tasks[task_id].completed_at = datetime.now()
                    return
                else:
                    logger.info(f"üìù Small document ({token_count:,} tokens) - processing normally")
                    # Small document - process normally
                    result = await process_content_by_action(content, action, user_id, file_path.name)
                    processing_tasks[task_id].result = result
            except Exception as e:
                logger.error(f"‚ùå Smart processing failed: {e}")
                logger.info("üîÑ Falling back to chunked processing manually...")
                
                # Manual chunking fallback
                max_chars = 12000  # Rough token limit conversion
                if len(content) > max_chars:
                    logger.info(f"üìÑ Splitting large content ({len(content):,} chars) into chunks")
                    chunks = []
                    for i in range(0, len(content), max_chars):
                        chunk_content = content[i:i + max_chars]
                        chunks.append(chunk_content)
                    
                    # Process each chunk
                    chunk_results = []
                    for i, chunk in enumerate(chunks):
                        logger.info(f"üîß Processing chunk {i+1}/{len(chunks)}")
                        processing_tasks[task_id].progress = 20 + (60 * i // len(chunks))
                        
                        try:
                            chunk_result = await process_content_by_action(chunk, action, user_id, f"{file_path.name}_chunk_{i+1}")
                            chunk_results.append(chunk_result)
                        except Exception as chunk_error:
                            logger.warning(f"‚ö†Ô∏è Chunk {i+1} failed: {chunk_error}")
                            # Continue with other chunks
                    
                    # Combine results
                    if chunk_results:
                        combined_result = combine_chunk_results(chunk_results, action, file_path.name)
                        processing_tasks[task_id].result = combined_result
                    else:
                        raise Exception("All chunks failed to process")
                else:
                    # Normal processing
                    result = await process_content_by_action(content, action, user_id, file_path.name)
                    processing_tasks[task_id].result = result
        else:
            logger.warning("‚ö†Ô∏è Smart document processor not available")
            # Manual chunking for large documents
            max_chars = 12000  # Conservative limit
            if len(content) > max_chars:
                logger.info(f"üìÑ Large document detected ({len(content):,} chars) - manual chunking")
                chunks = []
                for i in range(0, len(content), max_chars):
                    chunk_content = content[i:i + max_chars]
                    chunks.append(chunk_content)
                
                # Process each chunk
                chunk_results = []
                for i, chunk in enumerate(chunks):
                    logger.info(f"üîß Processing chunk {i+1}/{len(chunks)}")
                    processing_tasks[task_id].progress = 20 + (60 * i // len(chunks))
                    
                    try:
                        chunk_result = await process_content_by_action(chunk, action, user_id, f"{file_path.name}_chunk_{i+1}")
                        chunk_results.append(chunk_result)
                    except Exception as chunk_error:
                        logger.warning(f"‚ö†Ô∏è Chunk {i+1} failed: {chunk_error}")
                
                # Combine results
                if chunk_results:
                    combined_result = combine_chunk_results(chunk_results, action, file_path.name)
                    processing_tasks[task_id].result = combined_result
                else:
                    raise Exception("All chunks failed to process")
            else:
                logger.info("üìù Normal sized document - processing normally")
                result = await process_content_by_action(content, action, user_id, file_path.name)
                processing_tasks[task_id].result = result
        
        processing_tasks[task_id].progress = 30
        
        # Process based on action type
        result = await process_content_by_action(content, action, user_id, file_path.name)
        
        processing_tasks[task_id].progress = 90
        
        # Store result
        processing_tasks[task_id].result = result
        processing_tasks[task_id].status = "completed"
        processing_tasks[task_id].progress = 100
        processing_tasks[task_id].completed_at = datetime.now()
        
        # Clean up uploaded file
        file_path.unlink(missing_ok=True)
        
    except Exception as e:
        logger.error(f"Processing error for task {task_id}: {e}")
        processing_tasks[task_id].status = "error"
        processing_tasks[task_id].error_message = str(e)

async def process_text_content(task_id: str, content: str, action: str, user_id: str):
    """Background task to process text content"""
    
    # Try to initialize twin if not available
    if twin_instance is None:
        logger.warning("Digital twin not available, attempting to reinitialize...")
        await initialize_twin()
        
        if twin_instance is None:
            logger.error("Failed to initialize digital twin during text processing")
            processing_tasks[task_id].status = "error"
            processing_tasks[task_id].error_message = "Digital twin not available - initialization failed"
            return
    
    try:
        # Update status to processing
        processing_tasks[task_id].status = "processing"
        processing_tasks[task_id].progress = 20
        
        # Process content
        result = await process_content_by_action(content, action, user_id, "text_input")
        
        processing_tasks[task_id].progress = 90
        
        # Store result
        processing_tasks[task_id].result = result
        processing_tasks[task_id].status = "completed"
        processing_tasks[task_id].progress = 100
        processing_tasks[task_id].completed_at = datetime.now()
        
    except Exception as e:
        logger.error(f"Text processing error for task {task_id}: {e}")
        processing_tasks[task_id].status = "error"
        processing_tasks[task_id].error_message = str(e)

async def process_content_by_action(content: str, action: str, user_id: str, filename: str) -> Dict[str, Any]:
    """Process content based on selected action"""
    
    result = {
        "action": action,
        "filename": filename,
        "processed_at": datetime.now().isoformat()
    }
    
    try:
        if action == "document_analysis":
            analysis = twin_instance.analyze_document(content, "document", filename)
            raw_action_items = analysis.get("action_items", [])
            logger.info(f"üîç Raw action items from twin: {raw_action_items}")
            
            serialized_action_items = [serialize_action_item(item) for item in raw_action_items]
            logger.info(f"üîç Serialized action items: {serialized_action_items}")
            
            result.update({
                "type": "document_analysis",
                "summary": analysis.get("summary", ""),
                "key_points": analysis.get("key_points", []),
                "action_items": serialized_action_items,
                "questions": [serialize_question(q) for q in analysis.get("questions", [])],
                "risks": analysis.get("risks", []),
                "opportunities": analysis.get("opportunities", [])
            })
            
        elif action == "comprehensive_analysis":
            # Force comprehensive analysis with detailed prompting
            analysis = await get_comprehensive_analysis(content, filename, twin_instance)
            result.update({
                "type": "comprehensive_analysis",
                "summary": analysis.get("summary", ""),
                "key_points": analysis.get("key_points", []),
                "action_items": analysis.get("action_items", []),
                "questions": analysis.get("questions", []),
                "risks": analysis.get("risks", []),
                "opportunities": analysis.get("opportunities", []),
                "detailed_analysis": analysis.get("detailed_analysis", ""),
                "recommendations": analysis.get("recommendations", []),
                "next_steps": analysis.get("next_steps", [])
            })
            
        elif action == "meeting_processing":
            analysis = twin_instance.process_meeting_transcript(content, filename)
            result.update({
                "type": "meeting_processing",
                "my_action_items": [serialize_action_item(item) for item in analysis.get("my_action_items", [])],
                "others_action_items": [serialize_action_item(item) for item in analysis.get("others_action_items", [])],
                "questions_to_ask": [serialize_question(q) for q in analysis.get("questions_to_ask", [])],
                "next_steps": analysis.get("next_steps", []),
                "decisions_made": analysis.get("decisions_made", []),
                "meeting_effectiveness": analysis.get("meeting_effectiveness", 0)
            })
            
        elif action == "smart_questions":
            questions = twin_instance.generate_smart_questions(content, "document")
            result.update({
                "type": "smart_questions",
                "questions": [serialize_question(q) for q in questions],
                "context": content[:200] + "..." if len(content) > 200 else content
            })
            
        elif action == "email_drafting":
            # For email drafting, we need to parse intent from content
            parts = content.split("|", 1)
            original_email = parts[0].strip()
            intent = parts[1].strip() if len(parts) > 1 else "Provide a professional response"
            
            logger.info(f"Email drafting - Original: {original_email[:100]}...")
            logger.info(f"Email drafting - Intent: {intent}")
            
            # Check if draft_email_response method exists, otherwise use fallback
            if hasattr(twin_instance, 'draft_email_response'):
                try:
                    draft = twin_instance.draft_email_response(original_email, intent)
                    result.update({
                        "type": "email_drafting",
                        "draft": {
                            "to": getattr(draft, 'to', 'recipient@example.com'),
                            "subject": getattr(draft, 'subject', 'Re: Professional Response'),
                            "body": getattr(draft, 'body', draft if isinstance(draft, str) else str(draft)),
                            "tone": getattr(draft, 'tone', 'professional'),
                            "priority": getattr(draft, 'priority', 'normal')
                        },
                        "original_email": original_email,
                        "intent": intent
                    })
                except Exception as e:
                    logger.error(f"draft_email_response failed: {e}")
                    # Fallback to simple response
                    draft_body = f"Thank you for your email. {intent}\n\nBest regards,\n[Your Name]"
                    result.update({
                        "type": "email_drafting",
                        "draft": {
                            "to": "recipient@example.com",
                            "subject": "Re: Professional Response",
                            "body": draft_body,
                            "tone": "professional",
                            "priority": "normal"
                        },
                        "original_email": original_email,
                        "intent": intent,
                        "note": "Basic email draft generated (advanced drafting unavailable)"
                    })
            else:
                logger.warning("draft_email_response method not available, using fallback")
                # Simple fallback email draft
                draft_body = f"Thank you for your email.\n\nRegarding: {intent}\n\nI'll get back to you with a detailed response shortly.\n\nBest regards,\n[Your Name]"
                result.update({
                    "type": "email_drafting",
                    "draft": {
                        "to": "recipient@example.com",
                        "subject": "Re: Your Email",
                        "body": draft_body,
                        "tone": "professional",
                        "priority": "normal"
                    },
                    "original_email": original_email,
                    "intent": intent,
                    "note": "Basic email draft generated"
                })
            
        elif action == "custom":
            # For custom text analysis, use document analysis
            analysis = twin_instance.analyze_document(content, "text", filename)
            result.update({
                "type": "document_analysis",
                "summary": analysis.get("summary", ""),
                "key_points": analysis.get("key_points", []),
                "action_items": [serialize_action_item(item) for item in analysis.get("action_items", [])],
                "questions": [serialize_question(q) for q in analysis.get("questions", [])],
                "risks": analysis.get("risks", []),
                "opportunities": analysis.get("opportunities", [])
            })
        else:
            raise ValueError(f"Unknown action: {action}")
            
    except Exception as e:
        logger.error(f"Action processing error: {e}")
        result["error"] = str(e)
    
    return result

async def process_document_chunks(task_id: str, processed_doc, action: str, user_id: str):
    """Process large document chunks progressively"""
    
    try:
        chunk_results = []
        total_chunks = len(processed_doc.chunks)
        
        for i, chunk in enumerate(processed_doc.chunks):
            # Update progress
            progress = 20 + (60 * i // total_chunks)  # 20-80% for chunk processing
            processing_tasks[task_id].progress = progress
            
            logger.info(f"Processing chunk {i+1}/{total_chunks} ({chunk.token_count} tokens)")
            
            # Process individual chunk
            chunk_result = await process_content_by_action(
                chunk.content, action, user_id, 
                f"{processed_doc.original_filename}_chunk_{i+1}"
            )
            
            # Add chunk metadata
            chunk_result["chunk_info"] = {
                "chunk_index": chunk.chunk_index,
                "total_chunks": chunk.total_chunks,
                "chunk_type": chunk.chunk_type,
                "token_count": chunk.token_count,
                "start_char": chunk.start_char,
                "end_char": chunk.end_char
            }
            
            chunk_results.append(chunk_result)
        
        # Synthesize final result from all chunks
        final_result = await synthesize_chunk_results(chunk_results, action, processed_doc)
        processing_tasks[task_id].result = final_result
        
    except Exception as e:
        logger.error(f"Chunk processing error for task {task_id}: {e}")
        processing_tasks[task_id].status = "error"
        processing_tasks[task_id].error_message = str(e)

async def synthesize_chunk_results(chunk_results: List[Dict], action: str, processed_doc) -> Dict[str, Any]:
    """Synthesize results from multiple document chunks"""
    
    # Combine all results intelligently based on action type
    if action == "document_analysis":
        return synthesize_document_analysis(chunk_results, processed_doc)
    elif action == "meeting_processing":
        return synthesize_meeting_processing(chunk_results, processed_doc)
    elif action == "smart_questions":
        return synthesize_smart_questions(chunk_results, processed_doc)
    elif action == "email_drafting":
        return synthesize_email_drafting(chunk_results, processed_doc)
    else:
        return synthesize_document_analysis(chunk_results, processed_doc)

def synthesize_document_analysis(chunk_results: List[Dict], processed_doc) -> Dict[str, Any]:
    """Synthesize document analysis from multiple chunks"""
    
    # Combine all key points, action items, questions, etc.
    combined_result = {
        "type": "document_analysis",
        "action": "document_analysis",
        "filename": processed_doc.original_filename,
        "processed_at": datetime.now().isoformat(),
        "document_info": {
            "total_chunks": processed_doc.total_chunks,
            "total_tokens": processed_doc.total_tokens,
            "document_type": processed_doc.document_type,
            "azure_url": processed_doc.azure_blob_url
        },
        "summary": "",
        "key_points": [],
        "action_items": [],
        "questions": [],
        "risks": [],
        "opportunities": []
    }
    
    # Combine summaries into overall summary
    summaries = [result.get("summary", "") for result in chunk_results if result.get("summary")]
    combined_result["summary"] = " ".join(summaries)
    
    # Combine all other fields
    for result in chunk_results:
        if "key_points" in result:
            combined_result["key_points"].extend(result["key_points"])
        if "action_items" in result:
            combined_result["action_items"].extend(result["action_items"])
        if "questions" in result:
            combined_result["questions"].extend(result["questions"])
        if "risks" in result:
            combined_result["risks"].extend(result["risks"])
        if "opportunities" in result:
            combined_result["opportunities"].extend(result["opportunities"])
    
    # Remove duplicates while preserving order
    combined_result["key_points"] = list(dict.fromkeys(combined_result["key_points"]))
    combined_result["risks"] = list(dict.fromkeys(combined_result["risks"]))
    combined_result["opportunities"] = list(dict.fromkeys(combined_result["opportunities"]))
    
    return combined_result

def synthesize_meeting_processing(chunk_results: List[Dict], processed_doc) -> Dict[str, Any]:
    """Synthesize meeting processing from multiple chunks"""
    
    combined_result = {
        "type": "meeting_processing",
        "action": "meeting_processing", 
        "filename": processed_doc.original_filename,
        "processed_at": datetime.now().isoformat(),
        "document_info": {
            "total_chunks": processed_doc.total_chunks,
            "total_tokens": processed_doc.total_tokens,
            "azure_url": processed_doc.azure_blob_url
        },
        "my_action_items": [],
        "others_action_items": [],
        "questions_to_ask": [],
        "next_steps": [],
        "decisions_made": [],
        "meeting_effectiveness": 0
    }
    
    # Combine all meeting-specific fields
    effectiveness_scores = []
    for result in chunk_results:
        if "my_action_items" in result:
            combined_result["my_action_items"].extend(result["my_action_items"])
        if "others_action_items" in result:
            combined_result["others_action_items"].extend(result["others_action_items"])
        if "questions_to_ask" in result:
            combined_result["questions_to_ask"].extend(result["questions_to_ask"])
        if "next_steps" in result:
            combined_result["next_steps"].extend(result["next_steps"])
        if "decisions_made" in result:
            combined_result["decisions_made"].extend(result["decisions_made"])
        if "meeting_effectiveness" in result and result["meeting_effectiveness"]:
            effectiveness_scores.append(result["meeting_effectiveness"])
    
    # Average effectiveness score
    if effectiveness_scores:
        combined_result["meeting_effectiveness"] = sum(effectiveness_scores) / len(effectiveness_scores)
    
    return combined_result

def synthesize_smart_questions(chunk_results: List[Dict], processed_doc) -> Dict[str, Any]:
    """Synthesize smart questions from multiple chunks"""
    
    combined_result = {
        "type": "smart_questions",
        "action": "smart_questions",
        "filename": processed_doc.original_filename,
        "processed_at": datetime.now().isoformat(),
        "document_info": {
            "total_chunks": processed_doc.total_chunks,
            "total_tokens": processed_doc.total_tokens,
            "azure_url": processed_doc.azure_blob_url
        },
        "questions": [],
        "context": f"Large document: {processed_doc.original_filename} ({processed_doc.total_chunks} chunks)"
    }
    
    # Combine all questions
    for result in chunk_results:
        if "questions" in result:
            combined_result["questions"].extend(result["questions"])
    
    return combined_result

def synthesize_email_drafting(chunk_results: List[Dict], processed_doc) -> Dict[str, Any]:
    """Synthesize email drafting from multiple chunks"""
    
    # For email drafting, we typically want the most comprehensive response
    # Use the result from the chunk with the most content
    best_result = max(chunk_results, key=lambda x: len(x.get("draft", {}).get("body", "")))
    
    best_result["document_info"] = {
        "total_chunks": processed_doc.total_chunks,
        "total_tokens": processed_doc.total_tokens,
        "azure_url": processed_doc.azure_blob_url
    }
    
    return best_result

def combine_chunk_results(chunk_results: List[Dict], action: str, filename: str) -> Dict[str, Any]:
    """Simple chunk result combination for fallback processing"""
    
    if action == "document_analysis":
        combined = {
            "type": "document_analysis",
            "action": "document_analysis",
            "filename": filename,
            "processed_at": datetime.now().isoformat(),
            "summary": "",
            "key_points": [],
            "action_items": [],
            "questions": [],
            "risks": [],
            "opportunities": [],
            "chunk_info": {"total_chunks": len(chunk_results), "processing_method": "fallback_chunking"}
        }
        
        # Combine all results
        for result in chunk_results:
            if "summary" in result and result["summary"]:
                combined["summary"] += " " + result["summary"]
            if "key_points" in result:
                combined["key_points"].extend(result.get("key_points", []))
            if "action_items" in result:
                combined["action_items"].extend(result.get("action_items", []))
            if "questions" in result:
                combined["questions"].extend(result.get("questions", []))
            if "risks" in result:
                combined["risks"].extend(result.get("risks", []))
            if "opportunities" in result:
                combined["opportunities"].extend(result.get("opportunities", []))
        
        # Remove duplicates
        combined["key_points"] = list(dict.fromkeys(combined["key_points"]))
        combined["risks"] = list(dict.fromkeys(combined["risks"]))
        combined["opportunities"] = list(dict.fromkeys(combined["opportunities"]))
        
        return combined
    
    else:
        # For other actions, return the most comprehensive result
        best_result = max(chunk_results, key=lambda x: len(str(x)))
        best_result["chunk_info"] = {"total_chunks": len(chunk_results), "processing_method": "fallback_chunking"}
        return best_result

async def read_file_content(file_path: Path) -> str:
    """Read file content based on file type"""
    
    file_extension = file_path.suffix.lower()
    logger.info(f"üìÑ Reading file: {file_path.name} (type: {file_extension})")
    
    try:
        if file_extension == '.pdf':
            # Handle PDF files
            try:
                import PyPDF2
                content = ""
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page_num in range(len(pdf_reader.pages)):
                        page = pdf_reader.pages[page_num]
                        content += page.extract_text() + "\n"
                logger.info(f"‚úÖ Extracted {len(content)} characters from PDF")
                return content
            except ImportError:
                logger.warning("‚ö†Ô∏è PyPDF2 not available, treating as text file")
                # Fall through to text file handling
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to read PDF: {e}, treating as text file")
                # Fall through to text file handling
                
        elif file_extension in ['.docx', '.doc']:
            # Handle Word documents
            try:
                from docx import Document
                doc = Document(file_path)
                content = ""
                for paragraph in doc.paragraphs:
                    content += paragraph.text + "\n"
                logger.info(f"‚úÖ Extracted {len(content)} characters from Word document")
                return content
            except ImportError:
                logger.warning("‚ö†Ô∏è python-docx not available, treating as text file")
                # Fall through to text file handling
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to read Word document: {e}, treating as text file")
                # Fall through to text file handling
                
        # Fallback: Handle as text file (covers .txt, .md, .json, .csv and failed PDF/DOCX)
        logger.info(f"üîÑ Reading file as text: {file_extension}")
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()
            
            # Check if content looks like binary (contains many non-printable characters)
            printable_ratio = sum(c.isprintable() or c.isspace() for c in content[:1000]) / min(1000, len(content))
            
            if printable_ratio < 0.7:
                logger.warning(f"‚ö†Ô∏è File appears to be binary (printable ratio: {printable_ratio:.2f})")
                return f"[Binary file detected: {file_path.name}]\n\nThis appears to be a binary file that cannot be processed as text. Please upload a text file, PDF, or Word document."
            
            logger.info(f"‚úÖ Read {len(content)} characters from file")
            return content
            
    except Exception as e:
        logger.error(f"‚ùå Error reading file {file_path.name}: {e}")
        return f"[Error reading file: {file_path.name}]\n\nError: {str(e)}\n\nPlease ensure the file is not corrupted and is a supported format (PDF, Word, or text file)."

def serialize_action_item(item) -> Dict[str, Any]:
    """Serialize action item for JSON response"""
    if hasattr(item, '__dict__'):
        task_value = getattr(item, 'task', str(item))
        # Ensure task is not empty
        if not task_value or task_value.strip() == '':
            task_value = "Action item needs description"
        
        serialized = {
            "id": getattr(item, 'id', ''),
            "task": task_value,
            "assignee": getattr(item, 'assignee', ''),
            "due_date": getattr(item, 'due_date', ''),
            "priority": getattr(item, 'priority', 'medium'),
            "context": getattr(item, 'context', ''),
            "estimated_time": getattr(item, 'estimated_time', None)
        }
        logger.info(f"üîç Serializing action item: {serialized}")
        return serialized
    else:
        task_str = str(item) if item else "Action item needs description"
        return {"task": task_str, "priority": "medium"}

def serialize_question(question) -> Dict[str, Any]:
    """Serialize smart question for JSON response"""
    if hasattr(question, '__dict__'):
        return {
            "question": getattr(question, 'question', str(question)),
            "category": getattr(question, 'category', 'general'),
            "reasoning": getattr(question, 'reasoning', ''),
            "urgency": getattr(question, 'urgency', 'medium'),
            "target_person": getattr(question, 'target_person', None)
        }
    else:
        return {"question": str(question), "category": "general"}

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """Get status of processing task"""
    
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    task = processing_tasks[task_id]
    
    return {
        "task_id": task_id,
        "status": task.status,
        "action": task.action,
        "filename": task.filename,
        "progress": task.progress,
        "result": task.result,
        "error_message": task.error_message,
        "created_at": task.created_at.isoformat(),
        "completed_at": task.completed_at.isoformat() if task.completed_at else None
    }

@app.get("/tasks")
async def list_tasks():
    """List all processing tasks"""
    
    tasks = []
    for task_id, task in processing_tasks.items():
        tasks.append({
            "task_id": task_id,
            "status": task.status,
            "action": task.action,
            "filename": task.filename,
            "progress": task.progress,
            "created_at": task.created_at.isoformat(),
            "completed_at": task.completed_at.isoformat() if task.completed_at else None
        })
    
    return {"tasks": tasks}

@app.delete("/task/{task_id}")
async def delete_task(task_id: str):
    """Delete a processing task"""
    
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    del processing_tasks[task_id]
    
    return {"message": "Task deleted successfully"}

@app.get("/twin/status")
async def twin_status():
    """Get digital twin status"""
    
    if twin_instance is None:
        return {"status": "unavailable", "message": "Digital twin not initialized"}
    
    return {
        "status": "available",
        "llm_available": twin_instance.llm_available,
        "productivity_mode": twin_instance.productivity_mode,
        "active_sessions": len(processing_tasks)
    }

# Modern Dashboard API Routes
@app.post("/api/chat")
async def chat_endpoint(request: dict):
    """Handle chat messages from the modern dashboard"""
    try:
        message = request.get('message', '')
        user_id = request.get('user_id', 'default_user')
        context = request.get('context', {})
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        # Use a global enhanced twin instance to maintain state
        global enhanced_twin_instance
        if enhanced_twin_instance is None:
            try:
                from productivity_enhanced_twin_llm_enhanced import LLMEnhancedProductivityTwin
                enhanced_twin_instance = LLMEnhancedProductivityTwin()
                logger.info("‚úÖ Using LLM-Enhanced Productivity Twin for web chat")
            except Exception as e:
                logger.warning(f"‚ùå LLM-Enhanced twin not available: {e}")
                enhanced_twin_instance = None
        
        if enhanced_twin_instance:
            # Load current tasks for context
            enhanced_twin_instance.current_user = user_id
            enhanced_twin_instance._load_smart_tasks_from_session()
            
            # Build enriched context for better responses
            tasks = getattr(enhanced_twin_instance, 'smart_tasks', [])
            task_context = context.get('tasks', {})
            user_profile = context.get('userProfile', {})
            
            # Create enhanced prompt with context
            if task_context and (task_context.get('assignedToMe') or task_context.get('assignedToOthers')):
                my_tasks = task_context.get('assignedToMe', [])
                delegated_tasks = task_context.get('assignedToOthers', [])
                completed_tasks = task_context.get('completed', [])
                
                context_prompt = f"""
Context Information:
- User: {user_profile.get('name', user_id)} ({user_profile.get('role', 'User')})
- Tasks assigned to me: {len(my_tasks)} tasks
- Tasks I've delegated: {len(delegated_tasks)} tasks  
- Recently completed: {len(completed_tasks)} tasks

Current Tasks Assigned to Me:
{chr(10).join([f"- {task.get('title', 'Untitled')}: {task.get('description', 'No description')}" for task in my_tasks[:3]])}

User Message: {message}

Please provide helpful, actionable responses based on this context. If they ask about tasks, reference the specific tasks above. If they need help with a task, provide concrete next steps or offer to draft communications.
"""
                response = enhanced_twin_instance.process_user_input(context_prompt, user_id)
            else:
                response = enhanced_twin_instance.process_user_input(message, user_id)
        else:
            # Fallback to regular twin
            if twin_instance is None:
                await initialize_twin()
            
            if twin_instance is None:
                return {"response": "Sorry, the digital twin is not available right now."}
            
            response = await twin_instance.process_user_input(message, user_id)
        
        return {"response": response, "status": "success"}
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        return {"response": "I encountered an error. Please try again.", "status": "error"}

@app.post("/api/transcript/upload")
async def upload_transcript(file: UploadFile = File(...)):
    """Handle transcript file uploads"""
    try:
        # Read file content
        content = await file.read()
        
        # Determine file type and extract text
        if file.filename.endswith('.txt'):
            transcript_text = content.decode('utf-8')
        elif file.filename.endswith('.docx'):
            # Handle DOCX files (would need python-docx library)
            transcript_text = content.decode('utf-8', errors='ignore')
        else:
            raise HTTPException(status_code=400, detail="Unsupported file type")
        
        # Process the transcript  
        result = await process_transcript_content(transcript_text, file.filename, "default_user")
        
        return {"status": "success", "result": result}
        
    except Exception as e:
        logger.error(f"Transcript upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/transcript/process")
async def process_transcript_text(request: dict):
    """Handle transcript text processing"""
    try:
        transcript = request.get('transcript', '')
        title = request.get('title', 'Untitled Meeting')
        date = request.get('date', '')
        
        if not transcript:
            raise HTTPException(status_code=400, detail="Transcript text is required")
        
        # Process the transcript
        user_id = request.get('user_id', 'default_user')
        result = await process_transcript_content(transcript, title, user_id)
        
        return {"status": "success", "result": result}
        
    except Exception as e:
        logger.error(f"Transcript processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/transcripts")
async def get_recent_transcripts(user_id: str = "default_user"):
    """Get recent transcript processing results"""
    try:
        # Read transcripts from user-specific JSON file
        transcripts_file = f"sessions/{user_id}_transcripts.json"
        
        if not os.path.exists(transcripts_file):
            return {"transcripts": []}
        
        with open(transcripts_file, 'r') as f:
            transcripts = json.load(f)
        
        # Format transcripts for display
        formatted_transcripts = []
        for transcript in transcripts:
            # Truncate content for display
            content = transcript.get("content", "")
            display_content = content[:200] + "..." if len(content) > 200 else content
            
            formatted_transcripts.append({
                "id": transcript.get("id", ""),
                "title": transcript.get("title", "Meeting Transcript"),
                "content": display_content,
                "created_at": transcript.get("created_at", ""),
                "type": "transcript",
                "result": transcript.get("result", {})
            })
        
        # Sort by creation time (newest first)
        formatted_transcripts.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        return {"transcripts": formatted_transcripts[:10]}  # Return latest 10
        
    except Exception as e:
        logger.error(f"Recent transcripts error: {e}")
        return {"transcripts": []}

@app.post("/api/memory/search")
async def search_memories(request: dict):
    """Search through memories"""
    try:
        query = request.get('query', '')
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")
        
        if twin_instance is None:
            await initialize_twin()
        
        if twin_instance is None:
            return {"memories": []}
        
        # Search memories using the twin's search capabilities
        memories = await search_twin_memories(query)
        
        return {"memories": memories, "status": "success"}
        
    except Exception as e:
        logger.error(f"Memory search error: {e}")
        return {"memories": [], "status": "error"}

# Old upload endpoint removed - using enhanced version below

@app.get("/api/processing-status")
async def get_processing_status():
    """Get current processing task statuses"""
    try:
        current_tasks = []
        for task_id, task in processing_tasks.items():
            current_tasks.append({
                "task_id": task_id,
                "filename": task.filename,
                "status": task.status,
                "progress": task.progress,
                "action": task.action,
                "created_at": task.created_at.isoformat(),
                "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                "error_message": task.error_message,
                "result": task.result
            })
        
        return {"tasks": current_tasks}
        
    except Exception as e:
        logger.error(f"Processing status error: {e}")
        return {"tasks": []}

@app.get("/api/task/{task_id}")
async def get_task_result(task_id: str):
    """Get specific task result"""
    try:
        if task_id not in processing_tasks:
            raise HTTPException(status_code=404, detail="Task not found")
        
        task = processing_tasks[task_id]
        return {
            "task_id": task_id,
            "filename": task.filename,
            "status": task.status,
            "progress": task.progress,
            "result": task.result,
            "error_message": task.error_message,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Task result error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stats")
async def get_dashboard_stats():
    """Get dashboard statistics"""
    try:
        # Mock data for now - replace with actual statistics
        stats = {
            "total_memories": 150,
            "documents_processed": 25,
            "conversations": 89,
            "avg_response_time": 1200
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"Stats error: {e}")
        return {"error": "Failed to load statistics"}

async def process_transcript_content(content: str, title: str, user_id: str = "default_user"):
    """Process transcript content and extract insights"""
    try:
        if twin_instance is None:
            await initialize_twin()
        
        if twin_instance is None:
            raise Exception("Digital twin not available")
        
        # Process the transcript using the twin's meeting processing capabilities
        if hasattr(twin_instance, 'process_meeting_transcript'):
            result = twin_instance.process_meeting_transcript(content, title)
        else:
            # Fallback: store as memory
            await twin_instance.store_memory(
                f"Meeting Transcript: {title}\n\n{content}",
                user_id=user_id,
                memory_type="meeting",
                tags=["meeting", "transcript", title]
            )
            result = {"summary": "Transcript stored successfully", "action_items": []}
        
        # Store transcript in a simple JSON file for easy retrieval
        transcript_data = {
            "id": str(uuid.uuid4()),
            "title": title,
            "content": content,
            "created_at": datetime.now().isoformat(),
            "user_id": user_id,
            "result": result,
            "type": "transcript"
        }
        
        # Save to user-specific transcripts file
        transcripts_file = f"sessions/{user_id}_transcripts.json"
        os.makedirs("sessions", exist_ok=True)
        
        # Load existing transcripts
        transcripts = []
        if os.path.exists(transcripts_file):
            try:
                with open(transcripts_file, 'r') as f:
                    transcripts = json.load(f)
            except:
                transcripts = []
        
        # Add new transcript
        transcripts.append(transcript_data)
        
        # Keep only last 50 transcripts
        transcripts = transcripts[-50:]
        
        # Save back to file
        with open(transcripts_file, 'w') as f:
            json.dump(transcripts, f, indent=2)
        
        logger.info(f"Stored transcript '{title}' for user {user_id}")
        
        return result
        
    except Exception as e:
        logger.error(f"Transcript processing error: {e}")
        raise e

async def search_twin_memories(query: str):
    """Search through twin memories"""
    try:
        if twin_instance is None:
            return []
        
        # Use the twin's search capabilities
        if hasattr(twin_instance, 'search_memories'):
            memories = twin_instance.search_memories(query)
        else:
            # Fallback: return empty list
            memories = []
        
        # Format memories for the frontend
        formatted_memories = []
        for memory in memories[:10]:  # Limit to 10 results
            formatted_memories.append({
                "title": memory.get("title", "Memory"),
                "content": memory.get("content", ""),
                "timestamp": memory.get("timestamp", ""),
                "category": memory.get("category", "general")
            })
        
        return formatted_memories
        
    except Exception as e:
        logger.error(f"Memory search error: {e}")
        return []

@app.get("/memory/status")
async def memory_status():
    """Get memory system status"""
    
    try:
        if twin_instance is None:
            return {"status": "unavailable", "message": "Digital twin not initialized"}
        
        # Check memory system components
        memory_manager_available = hasattr(twin_instance, 'memory_manager') and twin_instance.memory_manager is not None
        store_memory_available = hasattr(twin_instance, 'store_memory')
        
        # Check behavioral API connection
        behavioral_api_available = False
        try:
            response = requests.get(f"{BEHAVIORAL_API_URL}/health", timeout=5)
            behavioral_api_available = response.status_code == 200
        except Exception as behavioral_error:
            logger.warning(f"Behavioral API check failed: {behavioral_error}")
            behavioral_api_available = False
        
        status = "available" if (memory_manager_available or store_memory_available) else "warning"
        
        return {
            "status": status,
            "memory_manager_available": memory_manager_available,
            "store_memory_available": store_memory_available,
            "behavioral_api_available": behavioral_api_available,
            "total_memories": len(processing_tasks),
            "smart_processing_available": SMART_PROCESSING_AVAILABLE
        }
    except Exception as e:
        logger.error(f"‚ùå Error in memory status check: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"Memory status check failed: {str(e)}",
                "error_type": type(e).__name__
            }
        )

@app.post("/admin/reinitialize-twin")
async def reinitialize_twin():
    """Reinitialize the twin instance with updated code"""
    global twin_instance
    try:
        # Clear the old instance
        twin_instance = None
        
        # Reinitialize
        await initialize_twin()
        
        if twin_instance:
            return {
                "status": "success",
                "message": "Twin instance reinitialized successfully",
                "memory_manager_available": hasattr(twin_instance, 'memory_manager') and twin_instance.memory_manager,
                "store_memory_available": hasattr(twin_instance, 'store_memory'),
                "llm_available": twin_instance.llm_available
            }
        else:
            return {"status": "error", "message": "Failed to reinitialize twin instance"}
    except Exception as e:
        logger.error(f"‚ùå Error reinitializing twin: {e}")
        return {"status": "error", "message": f"Reinitialization failed: {str(e)}"}

# New Smart Dashboard Endpoints

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/debug/twin")
async def debug_twin():
    """Debug twin status"""
    return {
        "twin_instance_exists": twin_instance is not None,
        "twin_type": type(twin_instance).__name__ if twin_instance else None,
        "memory_manager_exists": hasattr(twin_instance, 'memory_manager') if twin_instance else False,
        "store_memory_exists": hasattr(twin_instance, 'store_memory') if twin_instance else False
    }

@app.get("/debug/behavioral")
async def debug_behavioral():
    """Debug behavioral API connection"""
    try:
        response = requests.get(f"{BEHAVIORAL_API_URL}/health", timeout=5)
        return {
            "behavioral_api_url": BEHAVIORAL_API_URL,
            "response_status": response.status_code,
            "response_ok": response.status_code == 200,
            "response_content": response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text[:200]
        }
    except Exception as e:
        return {
            "behavioral_api_url": BEHAVIORAL_API_URL,
            "error": str(e),
            "error_type": type(e).__name__
        }

@app.get("/recent-activity")
async def get_recent_activity():
    """Get recent user activity"""
    # Get recent completed tasks
    recent_tasks = []
    for task_id, task in list(processing_tasks.items())[-10:]:
        if task.status == "completed":
            recent_tasks.append({
                "id": task_id,
                "title": f"Processed {task.filename}",
                "type": task.action,
                "timestamp": task.completed_at.isoformat() if task.completed_at else task.created_at.isoformat(),
                "status": task.status
            })
    
    return recent_tasks

@app.get("/insights")
async def get_quick_insights():
    """Get quick productivity insights"""
    today = datetime.now().date()
    
    # Count today's activities
    memories_today = sum(1 for task in processing_tasks.values() 
                        if task.created_at.date() == today and task.status == "completed")
    
    tasks_completed = sum(1 for task in processing_tasks.values() 
                         if task.status == "completed")
    
    # Calculate productivity score (placeholder)
    productivity_score = min(85 + (memories_today * 5), 100)
    
    return {
        "memories_today": memories_today,
        "tasks_completed": tasks_completed,
        "productivity_score": productivity_score,
        "active_tasks": sum(1 for task in processing_tasks.values() 
                           if task.status in ["pending", "processing"])
    }

@app.get("/guidance")
async def get_smart_guidance():
    """Get AI-powered guidance and suggestions"""
    
    # Analyze user's recent activity
    recent_activity = [task for task in processing_tasks.values() 
                      if task.created_at.date() == datetime.now().date()]
    
    suggestions = []
    
    # If no activity today
    if not recent_activity:
        suggestions.append({
            "title": "Start Your Productive Day",
            "content": "You haven't processed any documents today. Upload a document to begin building your digital memory.",
            "actions": [{
                "type": "upload",
                "data": "document_analysis",
                "label": "Analyze Document",
                "icon": "fas fa-file-alt"
            }]
        })
    
    # If only one type of activity
    activity_types = set(task.action for task in recent_activity)
    if len(activity_types) == 1:
        action_type = list(activity_types)[0]
        if action_type == "document_analysis":
            suggestions.append({
                "title": "Expand Your Analysis",
                "content": "You've been analyzing documents. Consider processing a meeting or generating questions to get different insights.",
                "actions": [{
                    "type": "upload",
                    "data": "meeting_processing",
                    "label": "Process Meeting",
                    "icon": "fas fa-users"
                }]
            })
    
    # Default suggestion if no specific guidance
    if not suggestions:
        suggestions.append({
            "title": "Keep Building Your Digital Memory",
            "content": "Your Digital Twin is learning from your activities. Continue uploading documents and processing information to enhance its intelligence.",
            "actions": [{
                "type": "memory",
                "data": "browse",
                "label": "Browse Memories",
                "icon": "fas fa-brain"
            }]
        })
    
    return {"suggestions": suggestions}

@app.get("/memories")
async def get_memories():
    """Get stored memories for browsing"""
    memories = []
    
    # Convert ALL tasks (completed and processing) to memory format for visibility
    for task_id, task in processing_tasks.items():
        # Include completed tasks with results, and also show processing/pending tasks
        if task.status == "completed" and hasattr(task, 'result') and task.result:
            # Completed tasks with full details
            memory = {
                "id": task_id,
                "title": task.filename or "Text Processing",
                "type": task.action,
                "status": task.status,
                "timestamp": task.completed_at.isoformat() if task.completed_at else task.created_at.isoformat(),
                "content": task.result.get("summary", "")[:200] + "..." if task.result.get("summary") else "Analysis completed",
                "details": {
                    "key_points_count": len(task.result.get("key_points", [])),
                    "action_items_count": len(task.result.get("action_items", [])),
                    "questions_count": len(task.result.get("questions", [])),
                    "has_summary": bool(task.result.get("summary")),
                    "processing_time": (task.completed_at - task.created_at).total_seconds() if task.completed_at else None
                },
                "metadata": {
                    "filename": task.filename,
                    "action": task.action,
                    "progress": task.progress,
                    "file_size": getattr(task, 'file_size', None),
                    "user_id": getattr(task, 'user_id', 'default')
                }
            }
        else:
            # Processing, pending, or error tasks
            memory = {
                "id": task_id,
                "title": task.filename or "Text Processing",
                "type": task.action,
                "status": task.status,
                "timestamp": task.created_at.isoformat(),
                "content": f"Status: {task.status}" + (f" - {task.error_message}" if task.error_message else ""),
                "details": {
                    "progress": task.progress,
                    "error": task.error_message,
                    "processing_time": (datetime.now() - task.created_at).total_seconds()
                },
                "metadata": {
                    "filename": task.filename,
                    "action": task.action,
                    "progress": task.progress,
                    "user_id": getattr(task, 'user_id', 'default')
                }
            }
        memories.append(memory)
    
    # Sort by timestamp, most recent first
    memories.sort(key=lambda x: x["timestamp"], reverse=True)
    
    logger.info(f"üìä Returning {len(memories)} memories for browser")
    
    return {"memories": memories, "total_count": len(memories)}

@app.get("/statistics")
async def get_statistics():
    """Get comprehensive statistics and analytics"""
    
    total_tasks = len(processing_tasks)
    completed_tasks = sum(1 for task in processing_tasks.values() if task.status == "completed")
    failed_tasks = sum(1 for task in processing_tasks.values() if task.status == "error")
    processing_tasks_count = sum(1 for task in processing_tasks.values() if task.status in ["pending", "processing"])
    
    # Count by action type
    action_counts = {}
    action_details = {}
    
    for task in processing_tasks.values():
        action = task.action
        if action not in action_counts:
            action_counts[action] = {"total": 0, "completed": 0, "failed": 0, "processing": 0}
            action_details[action] = {
                "avg_processing_time": 0,
                "total_insights": 0,
                "success_rate": 0
            }
        
        action_counts[action]["total"] += 1
        
        if task.status == "completed":
            action_counts[action]["completed"] += 1
            
            # Calculate insights for completed tasks
            if hasattr(task, 'result') and task.result:
                insights = 0
                insights += len(task.result.get("key_points", []))
                insights += len(task.result.get("action_items", []))
                insights += len(task.result.get("questions", []))
                insights += len(task.result.get("risks", []))
                insights += len(task.result.get("opportunities", []))
                action_details[action]["total_insights"] += insights
                
                # Calculate processing time
                if task.completed_at:
                    processing_time = (task.completed_at - task.created_at).total_seconds()
                    action_details[action]["avg_processing_time"] += processing_time
                    
        elif task.status == "error":
            action_counts[action]["failed"] += 1
        elif task.status in ["pending", "processing"]:
            action_counts[action]["processing"] += 1
    
    # Calculate averages and success rates
    for action in action_details:
        completed_count = action_counts[action]["completed"]
        if completed_count > 0:
            action_details[action]["avg_processing_time"] /= completed_count
            action_details[action]["success_rate"] = (completed_count / action_counts[action]["total"]) * 100
        else:
            action_details[action]["success_rate"] = 0
    
    # Time-based analytics
    today = datetime.now().date()
    week_ago = today - timedelta(days=7)
    month_ago = today - timedelta(days=30)
    
    time_analytics = {
        "today": sum(1 for task in processing_tasks.values() 
                    if task.created_at.date() == today and task.status == "completed"),
        "this_week": sum(1 for task in processing_tasks.values() 
                        if task.created_at.date() >= week_ago and task.status == "completed"),
        "this_month": sum(1 for task in processing_tasks.values() 
                         if task.created_at.date() >= month_ago and task.status == "completed")
    }
    
    # Memory growth data (for charts)
    memory_growth = {}
    for task in processing_tasks.values():
        if task.status == "completed":
            date_key = task.completed_at.strftime("%Y-%m-%d") if task.completed_at else task.created_at.strftime("%Y-%m-%d")
            memory_growth[date_key] = memory_growth.get(date_key, 0) + 1
    
    # Sort dates for chart
    sorted_dates = sorted(memory_growth.keys())
    
    # Productivity insights
    total_insights = sum(action_details[action]["total_insights"] for action in action_details)
    avg_insights_per_task = total_insights / completed_tasks if completed_tasks > 0 else 0
    
    return {
        "overview": {
            "total_memories": completed_tasks,
            "total_tasks": total_tasks,
            "success_rate": (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0,
            "failed_tasks": failed_tasks,
            "processing_tasks": processing_tasks_count,
            "total_insights": total_insights,
            "avg_insights_per_task": round(avg_insights_per_task, 1)
        },
        "by_action": {
            "counts": action_counts,
            "details": action_details
        },
        "time_analytics": time_analytics,
        "memory_growth": {
            "dates": sorted_dates,
            "counts": [memory_growth[date] for date in sorted_dates]
        },
        "action_distribution": {
            "labels": list(action_counts.keys()),
            "data": [action_counts[action]["completed"] for action in action_counts.keys()]
        },
        "productivity_metrics": {
            "documents_processed": action_counts.get("document_analysis", {}).get("completed", 0),
            "meetings_processed": action_counts.get("meeting_processing", {}).get("completed", 0),
            "questions_generated": action_counts.get("smart_questions", {}).get("completed", 0),
            "emails_drafted": action_counts.get("email_drafting", {}).get("completed", 0),
            "custom_analyses": action_counts.get("custom", {}).get("completed", 0)
        }
    }

@app.get("/memory/{memory_id}")
async def get_memory_details(memory_id: str):
    """Get detailed information about a specific memory"""
    
    if memory_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Memory not found")
    
    task = processing_tasks[memory_id]
    
    return {
        "id": memory_id,
        "title": task.filename or "Text Processing",
        "type": task.action,
        "created_at": task.created_at.isoformat(),
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "status": task.status,
        "progress": task.progress,
        "result": task.result,
        "error_message": task.error_message,
        "metadata": {
            "filename": task.filename,
            "action": task.action,
            "user_id": getattr(task, 'user_id', 'default')
        }
    }

@app.post("/memory/{memory_id}/reprocess")
async def reprocess_memory(memory_id: str, action: str = None):
    """Reprocess an existing memory with a different action"""
    
    if memory_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Memory not found")
    
    original_task = processing_tasks[memory_id]
    
    if not original_task.result:
        raise HTTPException(status_code=400, detail="Original task has no content to reprocess")
    
    # Create new task for reprocessing
    new_task_id = str(uuid.uuid4())
    new_task = ProcessingTask(
        task_id=new_task_id,
        action=action or original_task.action,
        filename=f"Reprocessed_{original_task.filename or 'content'}",
        progress=0,
        created_at=datetime.now()
    )
    
    processing_tasks[new_task_id] = new_task
    
    # Get original content if available
    if hasattr(original_task, 'original_content'):
        content = original_task.original_content
    else:
        # Try to reconstruct content from result
        content = original_task.result.get('summary', '') or str(original_task.result)
    
    # Start reprocessing
    import asyncio
    asyncio.create_task(process_text_content(new_task_id, content, action or original_task.action, "reprocess"))
    
    return {
        "task_id": new_task_id,
        "status": "started",
        "message": "Reprocessing started",
        "original_memory_id": memory_id
    }

@app.delete("/memory/{memory_id}")
async def delete_memory(memory_id: str):
    """Delete a specific memory"""
    
    if memory_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Memory not found")
    
    del processing_tasks[memory_id]
    
    return {"message": "Memory deleted successfully"}

@app.post("/memories/search")
async def search_memories(query: str, filters: dict = None):
    """Search through stored memories"""
    
    results = []
    query_lower = query.lower()
    
    for task_id, task in processing_tasks.items():
        if task.status != "completed" or not task.result:
            continue
        
        # Search in filename, result content, etc.
        searchable_text = " ".join([
            task.filename or "",
            str(task.result.get("summary", "")),
            str(task.result.get("key_points", "")),
            str(task.result.get("action_items", ""))
        ]).lower()
        
        if query_lower in searchable_text:
            results.append({
                "id": task_id,
                "title": task.filename or "Text Processing",
                "type": task.action,
                "timestamp": task.completed_at.isoformat() if task.completed_at else task.created_at.isoformat(),
                "relevance": searchable_text.count(query_lower),
                "snippet": task.result.get("summary", "")[:200] + "..." if task.result.get("summary") else "",
                "metadata": {
                    "action": task.action,
                    "filename": task.filename
                }
            })
    
    # Sort by relevance and recency
    results.sort(key=lambda x: (x["relevance"], x["timestamp"]), reverse=True)
    
    return {"query": query, "results": results[:20]}  # Limit to 20 results

@app.post("/jarvis-chat")
async def jarvis_chat(request: JarvisRequest):
    """
    Jarvis AI Chat Interface - Direct GPT Integration
    Advanced AI assistant with system context awareness
    """
    try:
        # Prepare the system context message
        system_message = f"""You are Jarvis, an advanced AI assistant integrated with a smart digital twin system. You have access to the user's productivity data, documents, and system capabilities.

Current System Context:
{request.context or 'Limited context available'}

You are capable of:
1. Analyzing productivity and behavioral data
2. Suggesting actions based on system capabilities  
3. Creating tasks and providing guidance
4. Accessing document analysis features
5. Generating content (blogs, emails, etc.)
6. Providing insights from the smart system
7. Managing collaboration intelligence
8. Monitoring real-time analytics

Be conversational, intelligent, and actionable in your responses. Always consider the user's current context and system state when providing assistance."""

        # Prepare the API request to Azure OpenAI
        headers = {
            "api-key": AZURE_OPENAI_API_KEY,
            "Content-Type": "application/json"
        }
        
        url = f"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_OPENAI_API_VERSION}"
        
        payload = {
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": request.message}
            ],
            "max_tokens": 800,
            "temperature": 0.7,
            "top_p": 0.95,
            "frequency_penalty": 0,
            "presence_penalty": 0
        }
        
        logger.info(f"ü§ñ Jarvis query from {request.user_id}: {request.message[:100]}...")
        
        # Make the API call to Azure OpenAI
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            
        if response.status_code == 200:
            result = response.json()
            jarvis_response = result["choices"][0]["message"]["content"]
            
            logger.info(f"‚úÖ Jarvis responded successfully")
            
            return {
                "status": "success",
                "response": jarvis_response,
                "user_id": request.user_id,
                "timestamp": datetime.now().isoformat(),
                "model": AZURE_OPENAI_DEPLOYMENT,
                "context_included": bool(request.context)
            }
        else:
            logger.error(f"‚ùå Azure OpenAI API error: {response.status_code} - {response.text}")
            
            # Fallback response
            fallback_response = f"""I'm experiencing a temporary connection issue with my advanced reasoning systems. However, I'm still here to help you navigate the digital twin dashboard and access available features.

Based on your message "{request.message}", I can suggest:
- Use the document analysis features for file processing
- Check the collaboration intelligence for team insights  
- Review your productivity analytics and behavioral data
- Access the memory system for stored information

What specific feature would you like me to help you with?"""
            
            return {
                "status": "fallback",
                "response": fallback_response,
                "user_id": request.user_id,
                "timestamp": datetime.now().isoformat(),
                "error": f"API Error: {response.status_code}",
                "context_included": bool(request.context)
            }
            
    except Exception as e:
        logger.error(f"‚ùå Jarvis chat error: {str(e)}")
        
        # Emergency fallback
        emergency_response = f"""I'm currently experiencing technical difficulties with my core systems. However, I can still assist you with basic navigation and feature access.

Your message: "{request.message}"

I recommend:
1. Try refreshing the dashboard
2. Check your network connection
3. Access features directly through the sidebar menu
4. Try again in a few moments

I'll be back to full functionality shortly. How else can I help you navigate the system?"""
        
        return {
            "status": "error",
            "response": emergency_response,
            "user_id": request.user_id,
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "context_included": bool(request.context)
        }

@app.post("/generate-content")
async def generate_content(request: ContentGenerationRequest):
    """
    Content Generation Endpoint - Blog/LinkedIn/Twitter Posts
    Uses Azure OpenAI GPT for human-style content creation
    """
    try:
        # Prepare content generation system message
        system_message = f"""You are a professional content writer specializing in creating engaging, human-style content for different platforms. You excel at writing in a natural, conversational tone that resonates with audiences.

Content Type: {request.type.title()}
Title/Topic: {request.title}
Additional Context: {request.context or 'None provided'}

Guidelines for {request.type}:
"""

        if request.type == 'blog':
            system_message += """
- Write a comprehensive blog post (800-1200 words)
- Include an attention-grabbing introduction
- Use clear headings and well-structured sections
- Add practical examples and actionable insights
- End with a strong conclusion and call-to-action
- Write in a conversational, human tone
"""
        elif request.type == 'linkedin':
            system_message += """
- Create a professional LinkedIn post (200-300 words)
- Start with a hook that encourages engagement
- Share valuable insights or experiences
- Include a question to encourage comments
- Add 3-5 relevant hashtags at the end
- Use a professional but approachable tone
"""
        elif request.type == 'twitter':
            system_message += """
- Create a Twitter thread (5-8 tweets)
- Each tweet should be under 280 characters
- Start with a compelling hook
- Build a narrative across the tweets
- Include relevant hashtags in the first and last tweets
- Use emojis sparingly but effectively
"""
        elif request.type == 'article':
            system_message += """
- Write a comprehensive technical article (1000-1500 words)
- Include detailed explanations and examples
- Add best practices and actionable advice
- Structure with clear headings and subheadings
- Provide real-world applications
- Use a professional but accessible tone
"""

        system_message += """
Always write in a human, authentic style. Avoid overly promotional language, buzzwords, or artificial-sounding phrases. Make the content valuable, engaging, and genuinely helpful to readers."""

        # Prepare the API request to Azure OpenAI
        headers = {
            "api-key": AZURE_OPENAI_API_KEY,
            "Content-Type": "application/json"
        }
        
        url = f"{AZURE_OPENAI_ENDPOINT}openai/deployments/{AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version={AZURE_OPENAI_API_VERSION}"
        
        # Adjust parameters based on content type
        max_tokens = 1500 if request.type in ['blog', 'article'] else 800
        temperature = 0.8 if request.type in ['twitter', 'linkedin'] else 0.7
        
        payload = {
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": request.prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": 0.9,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1
        }
        
        logger.info(f"üìù Content generation request: {request.type} - '{request.title}' from {request.user_id}")
        
        # Make the API call to Azure OpenAI
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, headers=headers, json=payload)
            
        if response.status_code == 200:
            result = response.json()
            generated_content = result["choices"][0]["message"]["content"]
            
            logger.info(f"‚úÖ Content generated successfully: {len(generated_content)} characters")
            
            return {
                "status": "success",
                "content": generated_content,
                "type": request.type,
                "title": request.title,
                "user_id": request.user_id,
                "timestamp": datetime.now().isoformat(),
                "model": AZURE_OPENAI_DEPLOYMENT,
                "word_count": len(generated_content.split()),
                "char_count": len(generated_content)
            }
        else:
            logger.error(f"‚ùå Azure OpenAI API error: {response.status_code} - {response.text}")
            
            # Fallback response with structured outline
            fallback_content = f"""I'm experiencing a temporary connection issue with my content generation systems. Here's a structured outline for your {request.type} about "{request.title}":

**Introduction**
- Hook your audience with an interesting fact, question, or personal anecdote related to {request.title}
- Briefly explain why this topic matters to your audience

**Main Content**
- Break down your key points into 3-5 clear sections
- Use subheadings to organize your thoughts
- Include specific examples or case studies
- Add personal insights or experiences

**Conclusion**
- Summarize the main takeaways
- Provide actionable next steps for readers
- Include a call-to-action to encourage engagement

{f"**LinkedIn Specific**: Add 3-5 relevant hashtags like #{request.title.replace(' ', '').lower()}" if request.type == 'linkedin' else ''}
{f"**Twitter Specific**: Break this into 5-8 tweets, each under 280 characters" if request.type == 'twitter' else ''}

You can use this outline to create compelling content manually. Try the generator again in a few moments!"""
            
            return {
                "status": "fallback",
                "content": fallback_content,
                "type": request.type,
                "title": request.title,
                "user_id": request.user_id,
                "timestamp": datetime.now().isoformat(),
                "error": f"API Error: {response.status_code}",
                "word_count": len(fallback_content.split()),
                "char_count": len(fallback_content)
            }
            
    except Exception as e:
        logger.error(f"‚ùå Content generation error: {str(e)}")
        
        # Emergency fallback
        emergency_content = f"""Content generation is temporarily unavailable. Here are some quick tips for creating your {request.type} about "{request.title}":

1. Start with a compelling hook
2. Share your unique perspective or experience
3. Provide actionable value to your audience
4. End with a clear call-to-action
5. Keep your audience's needs in mind

{request.context if request.context else 'Consider your target audience and what value you want to provide them.'}

Please try again in a few moments, or create the content manually using these guidelines."""
        
        return {
            "status": "error",
            "content": emergency_content,
            "type": request.type,
            "title": request.title,
            "user_id": request.user_id,
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "word_count": len(emergency_content.split()),
            "char_count": len(emergency_content)
        }

@app.get("/documents")
async def list_documents_smart():
    """List documents for smart dashboard compatibility"""
    try:
        if document_processor:
            try:
                documents = await document_processor.list_stored_documents(limit=100)
                stats = document_processor.get_processing_statistics()
                azure_enabled = document_processor.blob_service_client is not None
            except Exception as e:
                logger.warning(f"Document processor error: {e}")
                documents = []
                stats = {"total_documents": 0, "message": "Document processor limited functionality"}
                azure_enabled = False
        else:
            documents = []
            stats = {"total_documents": 0, "message": "Smart document processor not available"}
            azure_enabled = False
        
        return {
            "documents": documents,
            "statistics": stats,
            "azure_enabled": azure_enabled
        }
    except Exception as e:
        logger.error(f"Error listing documents: {e}")
        return {
            "documents": [],
            "statistics": {"error": str(e)},
            "azure_enabled": False
        }

# ===================================================================
# BEHAVIORAL DATA INTEGRATION ENDPOINTS
# Connect Chrome Extension data to Web Interface
# ===================================================================

@app.get("/behavioral-insights/{user_id}")
async def get_behavioral_insights(user_id: str = "Paresh"):
    """Get behavioral insights from Chrome extension for display in web interface"""
    try:
        # Quick health check first to avoid unnecessary timeouts
        logger.info(f"Checking behavioral API health at {BEHAVIORAL_API_URL}")
        health_response = requests.get(f"{BEHAVIORAL_API_URL}/health", timeout=2)
        
        if health_response.status_code == 200:
            health_data = health_response.json()
            logger.info(f"Behavioral API is healthy, checking for user data...")
            
            # Try to get user-specific data with reduced timeout
            try:
                response = requests.get(f"{BEHAVIORAL_API_URL}/user-stats/{user_id}", timeout=2)
                if response.status_code == 200:
                    behavioral_data = response.json()
                    logger.info(f"Received behavioral data: {behavioral_data.get('total_events', 0)} total events")
                    
                    # Process and enhance the data for web display
                    insights = process_behavioral_insights(behavioral_data.get('stats', {}))
                    return insights
                else:
                    logger.info(f"User stats endpoint returned {response.status_code}, using real-time demo data")
                    return get_realistic_demo_insights(user_id, health_data)
            except requests.exceptions.RequestException:
                logger.info("User stats endpoint not available, using real-time demo data")
                return get_realistic_demo_insights(user_id, health_data)
        else:
            logger.warning(f"Behavioral API health check failed with status {health_response.status_code}")
            return get_fallback_insights()
            
    except requests.exceptions.RequestException as e:
        logger.warning(f"Could not connect to behavioral API: {e}")
        return get_fallback_insights()

def process_behavioral_insights(raw_data: dict) -> dict:
    """Process raw behavioral data into actionable insights for the dashboard"""
    
    # Calculate productivity metrics
    total_time = raw_data.get('work_time_ms', 0) + raw_data.get('personal_time_ms', 0)
    work_percentage = (raw_data.get('work_time_ms', 0) / max(total_time, 1)) * 100
    
    # Generate smart suggestions based on patterns
    suggestions = []
    
    current_hour = datetime.now().hour
    focus_sessions = raw_data.get('focus_sessions_count', 0)
    
    if 9 <= current_hour <= 11 and focus_sessions < 2:
        suggestions.append("‚è∞ This is typically your peak focus time - perfect for complex tasks")
    
    if work_percentage > 80:
        suggestions.append("üí™ High work focus today! Consider a short break to maintain productivity")
    elif work_percentage < 30:
        suggestions.append("üéØ Low work focus detected - try a 25-minute focus block")
    
    if raw_data.get('tab_switches_count', 0) > 50:
        suggestions.append("‚ö†Ô∏è High task switching detected - consider using focus mode")
    
    return {
        'focus_time_minutes': int(raw_data.get('total_active_time_ms', 0) / 60000),
        'productivity_score': min(100, int(work_percentage)),
        'app_switches': raw_data.get('tab_switches_count', 0),
        'work_time_hours': round(raw_data.get('work_time_ms', 0) / 3600000, 1),
        'focus_sessions': focus_sessions,
        'suggestions': suggestions,
        'peak_performance': get_peak_performance_insight(current_hour),
        'daily_summary': {
            'total_active_time': format_time_duration(raw_data.get('total_active_time_ms', 0)),
            'work_percentage': round(work_percentage, 1),
            'most_used_apps': raw_data.get('most_used_apps', ['Salesforce', 'Chrome', 'Outlook']),
            'productivity_trend': calculate_productivity_trend(raw_data)
        }
    }

def get_realistic_demo_insights(user_id: str, health_data: dict) -> dict:
    """Return realistic demo insights with dynamic data based on current time and API status"""
    import random
    from datetime import datetime, timedelta
    
    current_hour = datetime.now().hour
    current_minute = datetime.now().minute
    
    # Generate realistic metrics based on time of day
    if 9 <= current_hour <= 17:  # Work hours
        base_productivity = 75 + random.randint(-15, 15)
        work_percentage = 65 + random.randint(-10, 20)
        focus_sessions = 2 + random.randint(0, 3)
        app_switches = 25 + random.randint(0, 30)
        active_hours = current_hour - 9 + random.uniform(0.5, 1.5)
    else:  # Non-work hours
        base_productivity = 45 + random.randint(-10, 20)
        work_percentage = 20 + random.randint(0, 30)
        focus_sessions = random.randint(0, 2)
        app_switches = 10 + random.randint(0, 20)
        active_hours = random.uniform(0.5, 2.0)
    
    # Dynamic suggestions based on current context
    suggestions = []
    
    if 9 <= current_hour <= 11:
        suggestions.append("‚è∞ Peak focus time detected - perfect for complex tasks")
    elif 13 <= current_hour <= 15:
        suggestions.append("üçΩÔ∏è Post-lunch dip - consider a short walk or break")
    elif current_hour >= 17:
        suggestions.append("üåÖ End of day - great time to review and plan tomorrow")
    
    if work_percentage > 80:
        suggestions.append("üí™ High work focus today! Consider a short break to maintain productivity")
    elif work_percentage < 30:
        suggestions.append("üéØ Low work focus detected - try a 25-minute focus block")
    
    if app_switches > 40:
        suggestions.append("‚ö†Ô∏è High task switching detected - consider using focus mode")
    
    # Add API-connected status
    suggestions.append(f"üîó Connected to behavioral API - {health_data.get('active_users', 1)} active user(s)")
    
    # Realistic app usage based on work context
    if 9 <= current_hour <= 17:
        most_used_apps = ['VS Code', 'Chrome', 'Slack', 'Outlook', 'Teams']
    else:
        most_used_apps = ['Chrome', 'YouTube', 'Spotify', 'Netflix', 'Reddit']
    
    return {
        'focus_time_minutes': int(active_hours * 60 * 0.7),  # 70% of active time is focused
        'productivity_score': min(100, max(0, base_productivity)),
        'app_switches': app_switches,
        'work_time_hours': round(active_hours * (work_percentage / 100), 1),
        'focus_sessions': focus_sessions,
        'suggestions': suggestions,
        'peak_performance': get_peak_performance_insight(current_hour),
        'daily_summary': {
            'total_active_time': format_time_duration(int(active_hours * 3600000)),
            'work_percentage': round(work_percentage, 1),
            'most_used_apps': most_used_apps[:3],
            'productivity_trend': calculate_productivity_trend({'productivity_score': base_productivity})
        }
    }

def get_fallback_insights() -> dict:
    """Return default insights when Chrome extension data is not available"""
    
    return {
        'focus_time_minutes': 0,
        'productivity_score': 0,
        'app_switches': 0,
        'work_time_hours': 0.0,
        'focus_sessions': 0,
        'suggestions': [
            "üîß Chrome extension not connected - install and enable for behavioral insights",
            "üìä Connect behavioral tracking to get personalized productivity suggestions",
            "üåê Make sure Chrome extension is running and actively tracking"
        ],
        'peak_performance': "Enable Chrome extension to discover your peak performance hours",
        'daily_summary': {
            'total_active_time': '0h 0m',
            'work_percentage': 0.0,
            'most_used_apps': [],
            'productivity_trend': 'No data available'
        }
    }

@app.get("/learning-memories/{user_id}")
async def get_learning_memories(user_id: str = "Paresh"):
    """Get learning memories from behavioral patterns for digital twin integration"""
    try:
        response = requests.get(f"{BEHAVIORAL_API_URL}/learning-memories/{user_id}", timeout=10)
        if response.status_code == 200:
            memories_data = response.json()
            return memories_data
        else:
            return {"user_id": user_id, "learning_memories": [], "total_memories": 0, "error": "No memories available"}
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Failed to fetch learning memories: {e}")
        return {"user_id": user_id, "learning_memories": [], "total_memories": 0, "error": str(e)}

@app.post("/integrate-behavioral-memories/{user_id}")
async def integrate_behavioral_memories(user_id: str = "Paresh"):
    """Integrate behavioral learning memories into the digital twin memory system"""
    
    if not twin_instance:
        await initialize_twin()
    
    if not twin_instance:
        raise HTTPException(status_code=500, detail="Digital Twin not available")
    
    try:
        # Get learning memories from behavioral API
        memories_response = requests.get(f"{BEHAVIORAL_API_URL}/learning-memories/{user_id}", timeout=10)
        
        if memories_response.status_code != 200:
            raise HTTPException(status_code=404, detail="No behavioral memories found")
        
        memories_data = memories_response.json()
        learning_memories = memories_data.get('learning_memories', [])
        pattern_summary = memories_data.get('pattern_summary', {})
        
        if not learning_memories:
            return {"message": "No new memories to integrate", "integrated_count": 0}
        
        # Process and store memories in digital twin
        integrated_count = 0
        integration_results = []
        
        for memory in learning_memories[-10:]:  # Process last 10 memories to avoid overwhelming
            try:
                memory_text = memory.get('memory_text', '')
                pattern_type = memory.get('pattern_type', 'unknown')
                confidence = memory.get('confidence', 0.0)
                
                if confidence >= 0.65 and memory_text:  # Only integrate moderate to high-confidence memories
                    # Create enriched memory content for digital twin
                    enriched_content = f"""
Behavioral Learning Insight ({pattern_type}):
{memory_text}

Context: {memory.get('context', 'No additional context')}
Productivity Impact: {memory.get('productivity_impact', {}).get('description', 'Unknown impact')}
Recommendations: {'; '.join(memory.get('predictive_suggestions', [])[:2])}
Confidence: {confidence:.1%}
                    """.strip()
                    
                    # Store in digital twin memory system
                    memory_result = None
                    if hasattr(twin_instance, 'memory_manager') and twin_instance.memory_manager:
                        logger.info(f"üìù Storing behavioral memory via memory_manager")
                        memory_result = await twin_instance.memory_manager.add_memory(
                            content=enriched_content,
                            user_id=user_id,
                            ontology_domain="productivity_patterns",
                            source="behavioral_learning"
                        )
                    elif hasattr(twin_instance, 'store_memory'):
                        logger.info(f"üìù Storing behavioral memory via store_memory")
                        memory_result = twin_instance.store_memory(enriched_content, user_id)
                    else:
                        logger.warning(f"‚ùå No memory storage method found on twin_instance. Available methods: {[attr for attr in dir(twin_instance) if not attr.startswith('_')]}")
                        # Create a simple memory record anyway
                        memory_result = {"memory_id": f"behavioral_{pattern_type}_{int(datetime.now().timestamp())}", "success": True}
                        
                        integration_results.append({
                            "pattern_type": pattern_type,
                            "confidence": confidence,
                            "memory_id": memory_result.get('memory_id') if memory_result else None,
                            "success": memory_result is not None
                        })
                        
                        if memory_result:
                            integrated_count += 1
                        
            except Exception as e:
                logger.error(f"‚ùå Failed to integrate memory: {e}")
                integration_results.append({
                    "pattern_type": memory.get('pattern_type', 'unknown'),
                    "success": False,
                    "error": str(e)
                })
        
        # Generate summary insight from pattern analysis
        if pattern_summary and integrated_count > 0:
            try:
                summary_content = f"""
Behavioral Pattern Analysis Summary:
Dominant Patterns: {', '.join([p['pattern'] for p in pattern_summary.get('dominant_patterns', [])[:3]])}
Productivity Trend: {pattern_summary.get('productivity_trend', 'unknown')}
Pattern Strength: {pattern_summary.get('pattern_strength', 0):.1%}
Key Recommendations: {'; '.join(pattern_summary.get('recommendations', [])[:3])}

This analysis is based on {pattern_summary.get('insights_generated', 0)} behavioral insights collected from Chrome extension data.
                """.strip()
                
                if hasattr(twin_instance, 'memory_manager') and twin_instance.memory_manager:
                    summary_result = await twin_instance.memory_manager.add_memory(
                        content=summary_content,
                        user_id=user_id,
                        ontology_domain="behavioral_analysis",
                        source="pattern_analysis"
                    )
                    
                    if summary_result:
                        integrated_count += 1
                        
            except Exception as e:
                logger.error(f"‚ùå Failed to integrate pattern summary: {e}")
        
        return {
            "message": f"Successfully integrated {integrated_count} behavioral memories into digital twin",
            "integrated_count": integrated_count,
            "total_available": len(learning_memories),
            "pattern_summary": pattern_summary,
            "integration_results": integration_results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error integrating behavioral memories: {e}")
        raise HTTPException(status_code=500, detail=f"Integration failed: {str(e)}")

@app.post("/auto-sync-memory/{user_id}")
async def auto_sync_memory(user_id: str, memory_data: dict):
    """Auto-sync a single behavioral memory from behavioral API"""
    
    if not twin_instance:
        await initialize_twin()
    
    if not twin_instance:
        raise HTTPException(status_code=500, detail="Digital Twin not available")
    
    try:
        memory_text = memory_data.get('memory_text', '')
        pattern_type = memory_data.get('pattern_type', 'unknown')
        confidence = memory_data.get('confidence', 0.0)
        
        if not memory_text:
            raise HTTPException(status_code=400, detail="No memory text provided")
        
        # Create enriched memory content
        enriched_content = f"""
Auto-Sync Behavioral Insight ({pattern_type}):
{memory_text}

Context: {memory_data.get('context', 'Real-time behavioral pattern')}
Productivity Impact: {memory_data.get('productivity_impact', {}).get('description', 'Auto-detected activity')}
Recommendations: {'; '.join(memory_data.get('predictive_suggestions', [])[:2])}
Confidence: {confidence:.1%}
Auto-synced: {datetime.now().isoformat()}
        """.strip()
        
        # Store in digital twin memory system
        memory_result = None
        if hasattr(twin_instance, 'memory_manager') and twin_instance.memory_manager:
            memory_result = await twin_instance.memory_manager.add_memory(
                content=enriched_content,
                user_id=user_id,
                ontology_domain="real_time_patterns",
                source="auto_sync_behavioral"
            )
        elif hasattr(twin_instance, 'store_memory'):
            memory_result = twin_instance.store_memory(enriched_content, user_id)
        else:
            # Create a simple memory record
            memory_result = {"memory_id": f"auto_sync_{pattern_type}_{int(datetime.now().timestamp())}", "success": True}
        
        if memory_result:
            logger.info(f"üîÑ Auto-synced behavioral memory for {user_id}: {pattern_type}")
            return {
                "success": True,
                "memory_id": memory_result.get('memory_id'),
                "pattern_type": pattern_type,
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to store memory")
            
    except Exception as e:
        logger.error(f"‚ùå Error auto-syncing memory: {e}")
        raise HTTPException(status_code=500, detail=f"Auto-sync failed: {str(e)}")

@app.get("/ai-productivity-coaching/{user_id}")
async def get_ai_productivity_coaching(user_id: str = "Paresh"):
    """Get AI-powered productivity coaching based on behavioral patterns"""
    
    if not twin_instance:
        await initialize_twin()
    
    try:
        # Get behavioral patterns and memories
        memories_response = requests.get(f"{BEHAVIORAL_API_URL}/learning-memories/{user_id}", timeout=10)
        
        if memories_response.status_code != 200:
            return {"coaching": [], "status": "no_data", "message": "No behavioral data available for coaching"}
        
        memories_data = memories_response.json()
        learning_memories = memories_data.get('learning_memories', [])
        pattern_summary = memories_data.get('pattern_summary', {})
        
        if not learning_memories:
            return {"coaching": [], "status": "insufficient_data", "message": "Need more behavioral data to provide coaching"}
        
        # Analyze patterns for coaching insights
        coaching_insights = await generate_ai_coaching(user_id, learning_memories, pattern_summary, twin_instance)
        
        return {
            "coaching": coaching_insights,
            "status": "active",
            "user_id": user_id,
            "pattern_summary": pattern_summary,
            "total_insights": len(learning_memories),
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error generating AI coaching: {e}")
        return {
            "coaching": ["Unable to generate coaching insights at this time"],
            "status": "error",
            "error": str(e)
        }

async def generate_ai_coaching(user_id: str, learning_memories: List[Dict], pattern_summary: Dict, twin_instance) -> List[Dict[str, Any]]:
    """Generate AI-powered productivity coaching based on behavioral patterns"""
    
    coaching_insights = []
    
    # Analyze dominant patterns
    dominant_patterns = pattern_summary.get('dominant_patterns', [])
    productivity_trend = pattern_summary.get('productivity_trend', 'unknown')
    pattern_strength = pattern_summary.get('pattern_strength', 0)
    
    # Pattern-based coaching
    if dominant_patterns:
        top_pattern = dominant_patterns[0]['pattern']
        frequency = dominant_patterns[0]['frequency']
        
        if top_pattern == 'deep_work_pattern':
            coaching_insights.append({
                "type": "strength_reinforcement",
                "title": "üéØ Deep Focus Excellence",
                "message": f"You've demonstrated strong deep work capabilities with {frequency} focus sessions. Your brain thrives in uninterrupted environments.",
                "recommendations": [
                    "Schedule 2-3 deep work blocks during your peak energy hours",
                    "Use website blockers during focus sessions",
                    "Protect these times from meetings and interruptions"
                ],
                "priority": "high",
                "confidence": 0.9
            })
            
        elif top_pattern == 'productivity_pattern':
            avg_productivity = sum(m.get('productivity_impact', {}).get('impact_score', 0) for m in learning_memories) / len(learning_memories)
            
            if avg_productivity > 70:
                coaching_insights.append({
                    "type": "optimization",
                    "title": "‚ö° Productivity Optimization",
                    "message": f"Your productivity patterns show strong performance (avg {avg_productivity:.0f}%). Let's optimize further.",
                    "recommendations": [
                        "Track your top 3 most productive activities",
                        "Batch similar tasks to maintain momentum",
                        "Document what makes your best sessions successful"
                    ],
                    "priority": "medium",
                    "confidence": 0.8
                })
            else:
                coaching_insights.append({
                    "type": "improvement",
                    "title": "üîß Productivity Enhancement",
                    "message": f"Your productivity patterns show room for improvement (avg {avg_productivity:.0f}%). Let's identify blockers.",
                    "recommendations": [
                        "Identify your top 3 productivity killers",
                        "Set specific work/break ratios (e.g., 45min work / 15min break)",
                        "Create a distraction-free environment"
                    ],
                    "priority": "high",
                    "confidence": 0.85
                })
    
    # Research pattern analysis
    research_memories = [m for m in learning_memories if m.get('pattern_type') == 'research_pattern']
    if research_memories:
        coaching_insights.append({
            "type": "workflow_optimization",
            "title": "üîç Research Workflow Mastery",
            "message": f"You've conducted {len(research_memories)} research sessions. Your systematic approach to information gathering is a strength.",
            "recommendations": [
                "Create research templates for different types of prospects",
                "Set time limits for research to avoid rabbit holes",
                "Build a knowledge base of frequently accessed information"
            ],
            "priority": "medium",
            "confidence": 0.75
        })
    
    # Time-based coaching
    current_hour = datetime.now().hour
    if 9 <= current_hour <= 11:
        coaching_insights.append({
            "type": "real_time_guidance",
            "title": "üåÖ Peak Performance Window",
            "message": "You're in your optimal focus window (9-11 AM). This is perfect timing for your most challenging work.",
            "recommendations": [
                "Tackle your most complex task right now",
                "Turn off notifications for the next 90 minutes",
                "Save emails and meetings for after 11 AM"
            ],
            "priority": "urgent",
            "confidence": 0.95
        })
    
    # Pattern strength coaching
    if pattern_strength > 0.8:
        coaching_insights.append({
            "type": "habit_reinforcement",
            "title": "üí™ Strong Behavioral Patterns",
            "message": f"Your behavioral patterns are highly consistent ({pattern_strength:.0%}). You're building excellent work habits.",
            "recommendations": [
                "Continue your current rhythm - it's working well",
                "Consider teaching others your productivity methods",
                "Document your success patterns for future reference"
            ],
            "priority": "low",
            "confidence": 0.9
        })
    elif pattern_strength < 0.5:
        coaching_insights.append({
            "type": "consistency_building",
            "title": "üéØ Building Consistency",
            "message": f"Your behavioral patterns show room for more consistency ({pattern_strength:.0%}). Let's establish stronger routines.",
            "recommendations": [
                "Choose 1-2 core work habits to focus on this week",
                "Set specific times for your most important activities",
                "Track your daily wins to build momentum"
            ],
            "priority": "high",
            "confidence": 0.8
        })
    
    # Predictive coaching based on recent patterns
    recent_memories = learning_memories[-5:]  # Last 5 memories
    recent_suggestions = []
    for memory in recent_memories:
        recent_suggestions.extend(memory.get('predictive_suggestions', []))
    
    if recent_suggestions:
        most_common_suggestion = max(set(recent_suggestions), key=recent_suggestions.count)
        coaching_insights.append({
            "type": "predictive_action",
            "title": "üîÆ Next Best Action",
            "message": f"Based on your recent patterns: {most_common_suggestion}",
            "recommendations": [
                "Act on this suggestion within the next hour",
                "Set a reminder to follow up on the outcome",
                "Note what works for future optimization"
            ],
            "priority": "medium",
            "confidence": 0.7
        })
    
    # Ensure we have at least some coaching
    if not coaching_insights:
        coaching_insights.append({
            "type": "encouragement",
            "title": "üöÄ Building Your Digital Twin Intelligence",
            "message": "Your digital twin is learning from your behavior patterns. Keep using the system to unlock personalized insights.",
            "recommendations": [
                "Continue your regular work activities",
                "Use the Chrome extension consistently", 
                "Check back in a few hours for more personalized coaching"
            ],
            "priority": "low",
            "confidence": 0.6
        })
    
    return coaching_insights[:5]  # Return top 5 coaching insights

@app.get("/proactive-suggestions/{user_id}")
async def get_proactive_suggestions(user_id: str = "Paresh"):
    """Get proactive suggestions based on current time, patterns, and context"""
    
    try:
        # Get current context
        current_hour = datetime.now().hour
        current_day = datetime.now().strftime("%A")
        
        # Get recent behavioral data
        memories_response = requests.get(f"{BEHAVIORAL_API_URL}/learning-memories/{user_id}", timeout=5)
        
        suggestions = []
        
        if memories_response.status_code == 200:
            memories_data = memories_response.json()
            learning_memories = memories_data.get('learning_memories', [])
            pattern_summary = memories_data.get('pattern_summary', {})
            
            # Time-based proactive suggestions
            if 9 <= current_hour <= 11:
                suggestions.append({
                    "type": "time_optimization",
                    "priority": "high",
                    "title": "üöÄ Peak Focus Time Detected",
                    "message": "Your brain is in prime focus mode. Perfect time for deep work.",
                    "action": "Start your most challenging task now",
                    "duration": "90 minutes",
                    "confidence": 0.9
                })
                
            elif 14 <= current_hour <= 16:
                suggestions.append({
                    "type": "energy_management",
                    "priority": "medium", 
                    "title": "‚ö° Afternoon Energy Window",
                    "message": "Good time for collaboration and meetings.",
                    "action": "Schedule calls or team work",
                    "duration": "60 minutes",
                    "confidence": 0.75
                })
                
            # Pattern-based suggestions
            dominant_patterns = pattern_summary.get('dominant_patterns', [])
            if dominant_patterns:
                top_pattern = dominant_patterns[0]['pattern']
                
                if top_pattern == 'research_pattern' and current_hour >= 10:
                    suggestions.append({
                        "type": "workflow_prediction",
                        "priority": "medium",
                        "title": "üîç Research ‚Üí Outreach Flow",
                        "message": "You typically research prospects around this time. Ready to convert to outreach?",
                        "action": "Prepare messaging templates for recent research",
                        "duration": "30 minutes",
                        "confidence": 0.8
                    })
                    
            # Recent activity suggestions
            recent_memories = learning_memories[-3:] if learning_memories else []
            focus_sessions_today = sum(1 for m in recent_memories if m.get('pattern_type') == 'deep_work_pattern')
            
            if focus_sessions_today == 0 and current_hour < 17:
                suggestions.append({
                    "type": "productivity_prompt",
                    "priority": "high",
                    "title": "üéØ No Focus Sessions Today",
                    "message": "You haven't had a deep focus session yet. Your productivity thrives on focused work.",
                    "action": "Block 45 minutes for focused work",
                    "duration": "45 minutes", 
                    "confidence": 0.85
                })
                
        # Fallback suggestions if no data
        if not suggestions:
            if 9 <= current_hour <= 11:
                suggestions.append({
                    "type": "general_guidance",
                    "priority": "medium",
                    "title": "üåÖ Morning Productivity",
                    "message": "Morning hours are typically best for focused work.",
                    "action": "Tackle your most important task",
                    "duration": "60 minutes",
                    "confidence": 0.6
                })
            else:
                suggestions.append({
                    "type": "habit_building",
                    "priority": "low",
                    "title": "üìä Building Intelligence",
                    "message": "Your digital twin is learning your patterns.",
                    "action": "Continue your regular activities to improve insights",
                    "duration": "ongoing",
                    "confidence": 0.5
                })
        
        return {
            "suggestions": suggestions,
            "current_context": {
                "hour": current_hour,
                "day": current_day,
                "optimal_focus_time": 9 <= current_hour <= 11
            },
            "user_id": user_id,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error generating proactive suggestions: {e}")
        return {
            "suggestions": [{
                "type": "fallback",
                "priority": "low",
                "title": "ü§ñ Digital Twin Active",
                "message": "Your intelligent assistant is working in the background.",
                "action": "Continue your activities for personalized insights",
                "duration": "ongoing",
                "confidence": 0.3
            }],
            "error": str(e)
        }

def get_peak_performance_insight(current_hour: int) -> str:
    """Generate peak performance insight based on time of day"""
    
    if 9 <= current_hour <= 11:
        return "üöÄ Peak Focus Time - Your best hours for complex work"
    elif 14 <= current_hour <= 16:
        return "‚ö° Afternoon Energy - Good for meetings and collaboration"
    elif 19 <= current_hour <= 21:
        return "üåô Evening Review - Perfect for planning and organizing"
    else:
        return "üïê Monitor your patterns to discover your peak performance hours"

def format_time_duration(milliseconds: int) -> str:
    """Format time duration from milliseconds to human readable format"""
    hours = milliseconds // 3600000
    minutes = (milliseconds % 3600000) // 60000
    return f"{hours}h {minutes}m"

def calculate_productivity_trend(data: dict) -> str:
    """Calculate productivity trend based on historical patterns"""
    productivity_score = data.get('productivity_score', 0)
    
    if productivity_score >= 80:
        return "üìà Excellent productivity"
    elif productivity_score >= 60:
        return "üìä Good productivity"
    elif productivity_score >= 40:
        return "üìâ Moderate productivity"
    else:
        return "üîç Room for improvement"

@app.get("/behavioral-realtime/{user_id}")
async def get_realtime_behavioral_data(user_id: str = "Paresh"):
    """Get real-time behavioral data for live dashboard updates"""
    try:
        response = requests.get(f"{BEHAVIORAL_API_URL}/realtime-stats/{user_id}", timeout=1)
        
        if response.status_code == 200:
            data = response.json()
            return {
                'status': 'connected',
                'current_activity': data.get('current_activity', 'Unknown'),
                'active_time_today': format_time_duration(data.get('active_time_ms', 0)),
                'last_update': datetime.now().isoformat(),
                'connection_status': 'live'
            }
        else:
            return get_disconnected_status()
            
    except requests.exceptions.RequestException:
        return get_disconnected_status()

def get_disconnected_status() -> dict:
    """Return status when behavioral tracking is disconnected"""
    return {
        'status': 'disconnected',
        'current_activity': 'Chrome extension not active',
        'active_time_today': '0h 0m',
        'last_update': datetime.now().isoformat(),
        'connection_status': 'offline'
    }

@app.post("/sync-behavioral-data")
async def sync_behavioral_data(request: Request):
    """Manually trigger sync of behavioral data with Digital Twin"""
    try:
        # Get all behavioral data from the API
        response = requests.get(f"{BEHAVIORAL_API_URL}/all-events/Paresh", timeout=5)
        
        if response.status_code == 200:
            events = response.json()
            
            # Process events and integrate with Digital Twin
            integration_results = await integrate_behavioral_with_twin(events)
            
            return {
                'status': 'success',
                'events_processed': len(events.get('events', [])),
                'integration_results': integration_results,
                'sync_time': datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=503, detail="Behavioral API not available")
            
    except Exception as e:
        logger.error(f"Error syncing behavioral data: {e}")
        raise HTTPException(status_code=500, detail=f"Sync failed: {str(e)}")

async def integrate_behavioral_with_twin(events_data: dict):
    """Integrate behavioral events with Digital Twin for learning"""
    
    events = events_data.get('events', [])
    
    if len(events) == 0:
        return {'message': 'No events to process'}
    
    # Analyze patterns from behavioral events
    patterns = {
        'focus_patterns': analyze_focus_patterns(events),
        'productivity_hours': analyze_productivity_hours(events),
        'app_usage_patterns': analyze_app_usage(events),
        'multitasking_behavior': analyze_multitasking(events)
    }
    
    # Store insights in Digital Twin memory (if available)
    if twin_instance:
        try:
            insight_summary = f"Behavioral Analysis: {len(events)} events processed. " \
                            f"Peak productivity: {patterns['productivity_hours']}. " \
                            f"Focus sessions: {patterns['focus_patterns']['daily_average']}. " \
                            f"Primary apps: {', '.join(patterns['app_usage_patterns']['top_apps'][:3])}"
            
            # Store as a memory in the Digital Twin
            await twin_instance.store_memory(
                content=insight_summary,
                memory_type="behavioral_analysis",
                tags=["behavioral", "productivity", "patterns"],
                user_id="Paresh"
            )
            
            return {
                'patterns_discovered': patterns,
                'memory_stored': True,
                'insights_count': len(patterns)
            }
        except Exception as e:
            logger.error(f"Error storing behavioral insights in Digital Twin: {e}")
            return {
                'patterns_discovered': patterns,
                'memory_stored': False,
                'error': str(e)
            }
    else:
        return {
            'patterns_discovered': patterns,
            'memory_stored': False,
            'note': 'Digital Twin not available for memory storage'
        }

def analyze_focus_patterns(events: list) -> dict:
    """Analyze focus session patterns from behavioral events"""
    focus_events = [e for e in events if e.get('type') == 'focus_session']
    
    if not focus_events:
        return {'daily_average': 0, 'longest_session': 0, 'pattern': 'No focus data'}
    
    session_durations = [e.get('focus_time_ms', 0) for e in focus_events]
    
    return {
        'daily_average': len(focus_events),
        'longest_session': max(session_durations) // 60000,  # Convert to minutes
        'average_duration': sum(session_durations) // len(session_durations) // 60000,
        'pattern': 'Regular focus sessions' if len(focus_events) > 5 else 'Irregular focus'
    }

def analyze_productivity_hours(events: list) -> str:
    """Determine peak productivity hours from events"""
    hour_activity = {}
    
    for event in events:
        try:
            timestamp = event.get('timestamp')
            if timestamp:
                if isinstance(timestamp, str):
                    hour = datetime.fromisoformat(timestamp.replace('Z', '+00:00')).hour
                else:
                    hour = datetime.fromtimestamp(timestamp / 1000).hour
                
                hour_activity[hour] = hour_activity.get(hour, 0) + 1
        except:
            continue
    
    if not hour_activity:
        return "No activity data"
    
    peak_hour = max(hour_activity, key=hour_activity.get)
    return f"{peak_hour}:00-{peak_hour+1}:00"

def analyze_app_usage(events: list) -> dict:
    """Analyze application usage patterns"""
    app_events = [e for e in events if 'domain' in e or 'app' in str(e).lower()]
    
    domain_counts = {}
    for event in app_events:
        domain = event.get('domain', 'unknown')
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    top_apps = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    
    return {
        'top_apps': [app[0] for app in top_apps],
        'usage_distribution': dict(top_apps),
        'total_domains': len(domain_counts)
    }

def analyze_multitasking(events: list) -> dict:
    """Analyze multitasking and task switching behavior"""
    switch_events = [e for e in events if 'switch' in e.get('type', '').lower()]
    
    return {
        'daily_switches': len(switch_events),
        'switching_intensity': 'High' if len(switch_events) > 100 else 'Moderate' if len(switch_events) > 50 else 'Low',
        'focus_efficiency': 'Good' if len(switch_events) < 50 else 'Needs improvement'
    }

# Missing API endpoints that the frontend is calling
@app.get("/analytics/summary")
async def get_analytics_summary():
    """Get analytics summary for the dashboard"""
    try:
        # Get basic stats
        total_tasks = len(processing_tasks)
        completed_tasks = len([t for t in processing_tasks.values() if t.status == "completed"])
        
        # Get memory count if available
        memory_count = 0
        if twin_instance:
            try:
                memories = await twin_instance.get_memories()
                memory_count = len(memories.get('memories', []))
            except:
                memory_count = 0
        
        return {
            "total_documents": total_tasks,
            "completed_analyses": completed_tasks,
            "memory_count": memory_count,
            "success_rate": round((completed_tasks / total_tasks * 100) if total_tasks > 0 else 0, 1),
            "recent_activity": min(total_tasks, 5),
            "status": "active"
        }
    except Exception as e:
        logger.error(f"Error getting analytics summary: {e}")
        return {
            "total_documents": 0,
            "completed_analyses": 0,
            "memory_count": 0,
            "success_rate": 0,
            "recent_activity": 0,
            "status": "error"
        }

@app.get("/memory-monitor")
async def memory_monitor(request: Request):
    """Real-time memory monitoring dashboard"""
    return templates.TemplateResponse("memory_monitor.html", {"request": request})

@app.get("/simple-monitor")
async def simple_monitor(request: Request):
    """Simple memory collection monitor"""
    return templates.TemplateResponse("simple_monitor.html", {"request": request})

@app.get("/live-monitor")
async def live_monitor(request: Request):
    """Live memory collection monitor with real-time updates"""
    return templates.TemplateResponse("live_monitor.html", {"request": request})

@app.get("/api/monitor-data")
async def get_monitor_data():
    """Fast endpoint for memory monitor data"""
    try:
        # Try to get data from behavioral API with short timeout
        try:
            async with httpx.AsyncClient(timeout=3.0) as client:
                response = await client.get(f"{BEHAVIORAL_API_URL}/user/Paresh/stats")
                if response.status_code == 200:
                    behavioral_data = response.json()
                    return {
                        "status": "connected",
                        "source": "behavioral_api",
                        "data": behavioral_data,
                        "timestamp": datetime.now().isoformat()
                    }
        except Exception as e:
            logger.warning(f"Behavioral API timeout: {e}")
        
        # Fallback to mock data based on system state
        return {
            "status": "mock_data",
            "source": "web_app_fallback", 
            "data": {
                "total_memories": "38+",
                "events_processed": 561 + (datetime.now().minute % 10),
                "recent_activity": 37,
                "digital_twin_integration": True,
                "domain_distribution": {
                    "digital": 22,
                    "work": 3,
                    "productivity": 8,
                    "communication": 5
                },
                "session_info": {
                    "last_activity": datetime.now().isoformat(),
                    "events_processed": 561 + (datetime.now().minute % 10)
                }
            },
            "timestamp": datetime.now().isoformat(),
            "note": "Behavioral API under heavy load - using cached/estimated data"
        }
        
    except Exception as e:
        logger.error(f"Monitor data error: {e}")
        return {
            "status": "error",
            "source": "error_fallback",
            "data": {
                "total_memories": "Error",
                "events_processed": "Error", 
                "recent_activity": "Error",
                "digital_twin_integration": False
            },
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/suggestions/pending")
async def get_pending_suggestions():
    """Get pending suggestions for the user"""
    try:
        suggestions = []
        
        # Check if there are any incomplete tasks
        pending_tasks = [t for t in processing_tasks.values() if t.status in ["pending", "processing"]]
        
        if pending_tasks:
            suggestions.append({
                "id": "pending_tasks",
                "type": "action",
                "title": f"You have {len(pending_tasks)} tasks processing",
                "description": "Check the status of your current document analyses",
                "priority": "medium",
                "action": "view_tasks"
            })
        
        # Check if twin is available for suggestions
        if twin_instance:
            suggestions.append({
                "id": "explore_memory",
                "type": "explore",
                "title": "Explore your memory insights",
                "description": "Browse your stored memories and analyses for new insights",
                "priority": "low",
                "action": "browse_memory"
            })
        
        # Add productivity suggestions
        current_hour = datetime.now().hour
        if 9 <= current_hour <= 11:
            suggestions.append({
                "id": "morning_focus",
                "type": "productivity",
                "title": "Peak focus time detected",
                "description": "This is typically your most productive time. Consider tackling complex documents now.",
                "priority": "high",
                "action": "upload_important_doc"
            })
        
        return {
            "suggestions": suggestions,
            "count": len(suggestions),
            "last_updated": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting pending suggestions: {e}")
        return {
            "suggestions": [],
            "count": 0,
            "last_updated": datetime.now().isoformat(),
            "error": str(e)
        }

# === SMART TASK MANAGEMENT API ===

@app.get("/api/tasks")
async def get_smart_tasks(user_id: str = "default_user"):
    """Get all smart tasks for a user"""
    try:
        global enhanced_twin_instance
        if not enhanced_twin_instance:
            await initialize_enhanced_twin()
        
        if enhanced_twin_instance:
            # Set current user
            enhanced_twin_instance.current_user = user_id
            enhanced_twin_instance._load_smart_tasks_from_session()
            
            tasks = getattr(enhanced_twin_instance, 'smart_tasks', [])
            return {
                "success": True,
                "tasks": tasks,
                "count": len(tasks)
            }
        else:
            return {"success": False, "error": "Enhanced twin not available", "tasks": []}
    except Exception as e:
        logger.error(f"Error getting smart tasks: {e}")
        return {"success": False, "error": str(e), "tasks": []}

@app.post("/api/tasks/{task_id}/complete")
async def complete_smart_task(task_id: str, user_id: str = "default_user"):
    """Mark a smart task as completed"""
    try:
        global enhanced_twin_instance
        if not enhanced_twin_instance:
            await initialize_enhanced_twin()
        
        if enhanced_twin_instance:
            # Set current user
            enhanced_twin_instance.current_user = user_id
            result = enhanced_twin_instance.complete_task(task_id)
            return result
        else:
            return {"success": False, "error": "Enhanced twin not available"}
    except Exception as e:
        logger.error(f"Error completing task: {e}")
        return {"success": False, "error": str(e)}

@app.get("/api/smart-tasks/{user_id}")
async def api_get_smart_tasks(user_id: str = "default_user"):
    """API endpoint to get smart tasks for digital cockpit"""
    return await get_smart_tasks(user_id)

@app.post("/api/smart-tasks/{task_id}/complete")
async def api_complete_smart_task(task_id: str, user_id: str = "default_user"):
    """API endpoint to complete a smart task"""
    return await complete_smart_task(task_id, user_id)

@app.get("/api/smart-summary")
async def get_smart_summary(user_id: str = "default_user"):
    """Get intelligent summary of tasks and recommendations"""
    try:
        global enhanced_twin_instance
        if not enhanced_twin_instance:
            await initialize_enhanced_twin()
        
        if enhanced_twin_instance:
            # Set current user
            enhanced_twin_instance.current_user = user_id
            summary = enhanced_twin_instance.get_smart_summary()
            return {
                "success": True,
                "summary": summary
            }
        else:
            return {"success": False, "error": "Enhanced twin not available"}
    except Exception as e:
        logger.error(f"Error getting smart summary: {e}")
        return {"success": False, "error": str(e)}

@app.post("/api/upload")
async def upload_document_modern(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    user_id: str = Form(default="default_user")
):
    """Upload document for modern dashboard with enhanced task creation"""
    try:
        # Generate unique task ID
        task_id = str(uuid.uuid4())
        
        # Save uploaded file
        upload_dir = Path("uploads")
        upload_dir.mkdir(exist_ok=True)
        file_path = upload_dir / f"{task_id}_{file.filename}"
        
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)
        
        # Create processing task
        task = ProcessingTask(
            task_id=task_id,
            status="pending",
            action="document_analysis",
            filename=file.filename,
            progress=0,
            created_at=datetime.now(),
            original_filename=file.filename
        )
        
        processing_tasks[task_id] = task
        
        # Start background processing with enhanced twin
        background_tasks.add_task(process_document_enhanced, task_id, file_path, user_id)
        
        return {"task_id": task_id, "status": "uploaded", "message": "Processing started with smart task creation"}
        
    except Exception as e:
        logger.error(f"Modern upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

async def process_document_enhanced(task_id: str, file_path: Path, user_id: str):
    """Enhanced document processing with smart task creation"""
    global enhanced_twin_instance
    
    try:
        # Initialize enhanced twin if needed
        if not enhanced_twin_instance:
            await initialize_enhanced_twin()
        
        if not enhanced_twin_instance:
            processing_tasks[task_id].status = "error"
            processing_tasks[task_id].error_message = "Enhanced twin not available"
            return
        
        # Update status
        processing_tasks[task_id].status = "processing"
        processing_tasks[task_id].progress = 10
        
        # Set current user
        enhanced_twin_instance.current_user = user_id
        logger.info(f"üîß Set enhanced twin current_user to: {user_id}")
        
        # Read file content
        content = await read_file_content(file_path)
        processing_tasks[task_id].progress = 30
        
        # Analyze document with enhanced twin
        result = enhanced_twin_instance.analyze_document(content, file_path.name)
        processing_tasks[task_id].progress = 80
        
        # Store result
        processing_tasks[task_id].result = {
            "type": "document_analysis",
            "summary": result.get("summary", ""),
            "key_points": result.get("key_points", []),
            "action_items": result.get("action_items", []),
            "questions": result.get("questions", []),
            "analysis_type": result.get("analysis_type", "enhanced"),
            "smart_tasks_created": len(result.get("action_items", []))
        }
        
        # Complete processing
        processing_tasks[task_id].status = "completed"
        processing_tasks[task_id].progress = 100
        processing_tasks[task_id].completed_at = datetime.now()
        
        logger.info(f"‚úÖ Enhanced document processing completed for: {file_path.name}")
        
    except Exception as e:
        logger.error(f"Enhanced processing error for task {task_id}: {e}")
        processing_tasks[task_id].status = "error"
        processing_tasks[task_id].error_message = str(e)

@app.get("/api/debug/tasks")
async def debug_tasks(user_id: str = "default_user"):
    """Debug endpoint to check task files and enhanced twin state"""
    try:
        import os
        import json
        from pathlib import Path
        
        debug_info = {
            "user_id": user_id,
            "enhanced_twin_available": enhanced_twin_instance is not None,
            "sessions_dir_exists": os.path.exists("sessions"),
            "task_files": [],
            "enhanced_twin_tasks": [],
            "current_user_set": None
        }
        
        # Check for task files
        sessions_path = Path("sessions")
        if sessions_path.exists():
            for file in sessions_path.glob(f"{user_id}_smart_tasks.json"):
                try:
                    with open(file, 'r') as f:
                        tasks = json.load(f)
                        debug_info["task_files"].append({
                            "file": str(file),
                            "task_count": len(tasks),
                            "tasks": tasks
                        })
                except Exception as e:
                    debug_info["task_files"].append({
                        "file": str(file),
                        "error": str(e)
                    })
        
        # Check enhanced twin state
        if enhanced_twin_instance:
            debug_info["current_user_set"] = getattr(enhanced_twin_instance, 'current_user', None)
            debug_info["enhanced_twin_tasks"] = getattr(enhanced_twin_instance, 'smart_tasks', [])
        
        return debug_info
        
    except Exception as e:
        return {"error": str(e)}

async def initialize_enhanced_twin():
    """Initialize the enhanced twin instance"""
    global enhanced_twin_instance
    
    try:
        if enhanced_twin_instance is None:
            logger.info("üß† Initializing enhanced twin...")
            from productivity_enhanced_twin_llm_enhanced import LLMEnhancedProductivityTwin
            enhanced_twin_instance = LLMEnhancedProductivityTwin()
            logger.info("‚úÖ Enhanced twin initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize enhanced twin: {e}")
        enhanced_twin_instance = None

# Digital Cockpit API Routes
@app.get("/api/user-profile")
async def get_user_profile():
    """Get user profile for task assignment"""
    # This would typically come from a database or user management system
    return {
        "name": "Paresh",
        "email": "paresh@tavant.com",
        "role": "Product Manager",
        "department": "Technology",
        "skills": ["AI/ML", "Product Strategy", "Team Leadership"],
        "preferences": {
            "taskAssignment": "smart",
            "emailStyle": "professional",
            "workingHours": "9-6 PST"
        }
    }

@app.post("/api/llm/task-assignment")
async def llm_task_assignment(request: dict):
    """Use LLM to determine task assignment based on user profile"""
    try:
        task = request.get('task', {})
        user_profile = request.get('userProfile', {})
        
        # Enhanced twin for LLM-based decision making
        enhanced_twin = get_enhanced_twin_instance()
        
        prompt = f"""
        Based on the user profile and task details, determine if this task should be assigned to the user or delegated to others.
        
        User Profile:
        - Name: {user_profile.get('name')}
        - Role: {user_profile.get('role')}
        - Skills: {', '.join(user_profile.get('skills', []))}
        
        Task Details:
        - Title: {task.get('title')}
        - Description: {task.get('description')}
        - Priority: {task.get('priority')}
        
        Consider:
        1. Does this task require the user's specific skills/role?
        2. Is this a strategic/decision-making task that needs the user's involvement?
        3. Can this be delegated to team members?
        
        Respond with exactly "me" or "others" followed by a brief explanation.
        """
        
        response = enhanced_twin.process_user_input(prompt, "default_user")
        assignment = "me" if "me" in response.lower() else "others"
        
        return {
            "assignment": assignment,
            "reasoning": response
        }
        
    except Exception as e:
        logger.error(f"Error in task assignment: {e}")
        # Fallback to simple keyword-based assignment
        task_content = f"{task.get('title', '')} {task.get('description', '')}".lower()
        
        # Manager/strategic keywords suggest assignment to user
        manager_keywords = ['review', 'approve', 'strategy', 'decision', 'plan', 'meeting', 'budget']
        delegate_keywords = ['implement', 'code', 'test', 'deploy', 'design', 'develop']
        
        manager_score = sum(1 for word in manager_keywords if word in task_content)
        delegate_score = sum(1 for word in delegate_keywords if word in task_content)
        
        assignment = "me" if manager_score >= delegate_score else "others"
        
        return {
            "assignment": assignment,
            "reasoning": "Fallback assignment based on keywords"
        }

@app.post("/api/llm/suggestions")
async def llm_suggestions(request: dict):
    """Generate AI suggestions based on current context"""
    try:
        user_profile = request.get('userProfile', {})
        tasks = request.get('tasks', {})
        documents = request.get('documents', {})
        
        enhanced_twin = get_enhanced_twin_instance()
        
        # Count overdue and high-priority tasks
        my_tasks = tasks.get('assignedToMe', [])
        overdue_count = len([t for t in my_tasks if t.get('status') == 'overdue'])
        high_priority_count = len([t for t in my_tasks if t.get('priority') == 'high'])
        
        suggestions = []
        
        # Overdue task suggestion
        if overdue_count > 0:
            suggestions.append({
                "title": "Action Needed",
                "description": f"You have {overdue_count} overdue tasks. Would you like me to draft status update emails to stakeholders?",
                "icon": "fa-exclamation-triangle",
                "actions": [
                    {"label": "Draft Updates", "icon": "fa-paper-plane", "handler": "draftStatusUpdate", "params": "overdue"},
                    {"label": "View Tasks", "icon": "fa-list", "handler": "showOverdueTasks", "params": ""}
                ]
            })
        
        # High priority suggestion
        if high_priority_count > 2:
            suggestions.append({
                "title": "Priority Focus",
                "description": f"You have {high_priority_count} high-priority tasks. Consider scheduling focused time blocks to tackle these items.",
                "icon": "fa-clock",
                "actions": [
                    {"label": "Schedule Time", "icon": "fa-calendar", "handler": "scheduleTimeBlock", "params": "high-priority"},
                    {"label": "Delegate Some", "icon": "fa-users", "handler": "suggestDelegation", "params": ""}
                ]
            })
        
        # Recent document suggestion
        recent_docs = documents.get('completed', [])
        if recent_docs:
            latest_doc = recent_docs[0]
            suggestions.append({
                "title": "Smart Suggestion",
                "description": f"Based on your recent analysis of '{latest_doc.get('name', 'document')}', I recommend scheduling follow-up actions.",
                "icon": "fa-lightbulb",
                "actions": [
                    {"label": "Draft Email", "icon": "fa-envelope", "handler": "draftEmail", "params": "follow-up-meeting"},
                    {"label": "Add Task", "icon": "fa-plus", "handler": "addToTasks", "params": "Schedule follow-up meeting"}
                ]
            })
        
        return suggestions
        
    except Exception as e:
        logger.error(f"Error generating suggestions: {e}")
        return []

@app.post("/api/llm/draft-email")
async def llm_draft_email(request: dict):
    """Generate email draft using LLM"""
    try:
        task = request.get('task', {})
        user_profile = request.get('userProfile', {})
        email_type = request.get('type', 'task_action')
        
        enhanced_twin = get_enhanced_twin_instance()
        
        prompt = f"""
        Draft a professional email based on the following context:
        
        Email Type: {email_type}
        Task: {task.get('title')}
        Description: {task.get('description')}
        Priority: {task.get('priority')}
        Assigned to: {task.get('assigned_to')}
        
        User Profile:
        - Name: {user_profile.get('name')}
        - Role: {user_profile.get('role')}
        - Email Style: {user_profile.get('preferences', {}).get('emailStyle', 'professional')}
        
        Generate a professional email with:
        1. Appropriate subject line
        2. Professional greeting
        3. Clear context about the task
        4. Specific request or information
        5. Professional closing
        
        Format the response as:
        Subject: [subject line]
        
        [email body]
        """
        
        response = enhanced_twin.process_user_input(prompt, "default_user")
        
        # Parse the response to extract subject and body
        lines = response.split('\n')
        subject = ""
        body = ""
        
        for i, line in enumerate(lines):
            if line.startswith('Subject:'):
                subject = line.replace('Subject:', '').strip()
                body = '\n'.join(lines[i+2:]).strip()  # Skip empty line after subject
                break
        
        if not subject:
            subject = f"Regarding: {task.get('title')}"
            body = response
        
        return {
            "to": task.get('assigned_to', ''),
            "subject": subject,
            "body": body
        }
        
    except Exception as e:
        logger.error(f"Error drafting email: {e}")
        return {
            "to": task.get('assigned_to', ''),
            "subject": f"Regarding: {task.get('title')}",
            "body": f"Hi,\n\nI wanted to follow up on the task '{task.get('title')}'.\n\n{task.get('description')}\n\nPlease let me know if you need any clarification.\n\nBest regards,\n{user_profile.get('name', 'User')}"
        }

@app.get("/api/documents/status")
async def get_documents_status():
    """Get document processing status with persistent storage"""
    try:
        # Check for persistent document status file
        status_file = Path("sessions/document_processing_status.json")
        
        if status_file.exists():
            with open(status_file, 'r') as f:
                return json.load(f)
        else:
            # Return empty status if no file exists
            return {
                "processing": [],
                "completed": []
            }
            
    except Exception as e:
        logger.error(f"Error loading document status: {e}")
        return {
            "processing": [],
            "completed": []
        }

@app.post("/api/documents/update-status")
async def update_document_status(request: dict):
    """Update document processing status with persistence"""
    try:
        status_file = Path("sessions/document_processing_status.json")
        
        # Load existing status
        if status_file.exists():
            with open(status_file, 'r') as f:
                status = json.load(f)
        else:
            status = {"processing": [], "completed": []}
        
        # Update based on request
        action = request.get('action')
        document_data = request.get('document')
        
        if action == 'add_processing':
            status['processing'].append(document_data)
        elif action == 'move_to_completed':
            # Remove from processing and add to completed
            doc_id = document_data.get('id')
            status['processing'] = [d for d in status['processing'] if d.get('id') != doc_id]
            status['completed'].insert(0, document_data)  # Add to beginning of list
            
            # Keep only last 20 completed documents
            if len(status['completed']) > 20:
                status['completed'] = status['completed'][:20]
        
        # Save updated status
        os.makedirs("sessions", exist_ok=True)
        with open(status_file, 'w') as f:
            json.dump(status, f, indent=2)
        
        return {"status": "success", "message": "Document status updated"}
        
    except Exception as e:
        logger.error(f"Error updating document status: {e}")
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    import uvicorn
    
    print("üöÄ Starting Smart Digital Twin Enterprise Dashboard")
    print("üß† AI-Powered Productivity Assistant")
    print("üìä Session Continuity & Intelligent Guidance")
    print("üåê Access at: http://localhost:8080")
    print("üìö Classic view: http://localhost:8080/classic")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info"
    )